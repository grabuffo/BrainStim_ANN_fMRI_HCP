{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grabuffo/BrainStim_ANN_fMRI_HCP/blob/main/notebooks/Process_TMS_fMRI_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
     "cells": [
      {
       "cell_type": "markdown",
       "id": "a5550615",
       "metadata": {},
       "source": [
        "# Process TMS-fMRI data (task-rest) for population ANN\n",
        "\n",
        "This notebook loads the preprocessed **TMS-fMRI** parcel time series stored in:\n",
        "\n",
        "- `data/TMS_fMRI/dataset_tian50_schaefer400_allruns.pkl`\n",
        "\n",
        "and creates, **per subject**, the arrays:\n",
        "\n",
        "- `*_signals.npy`  (T × 450)\n",
        "- `*_inputs.npy`   ((T−S) × (S·450))\n",
        "- `*_targets.npy`  ((T−S) × 450)\n",
        "\n",
        "where **S** is the number of past steps used as input (multi-to-one).\n",
        "\n",
        "Important design choices:\n",
        "- **No concatenation across subjects before building samples** (avoids artificial transitions).\n",
        "- Concatenation happens **within subject** across all available **task-rest** runs.\n",
        "- Filtering is applied **per run** before concatenation to avoid boundary artifacts.\n"
       ]
      },
      {
       "cell_type": "code",
       "execution_count": null,
       "id": "a7defc24",
       "metadata": {},
       "outputs": [],
       "source": [
        "# --- Setup ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import os, sys, json, pickle\n",
        "import numpy as np\n",
        "\n",
        "# Project + paths\n",
        "PROJECT_DIR = \"/content/drive/MyDrive/Colab Notebooks/Brain_Stim_ANN\"\n",
        "DATA_DIR    = os.path.join(PROJECT_DIR, \"data\")\n",
        "\n",
        "TMS_PKL_PATH = os.path.join(DATA_DIR, \"TMS_fMRI\", \"dataset_tian50_schaefer400_allruns.pkl\")\n",
        "OUT_DIR      = os.path.join(DATA_DIR, \"preprocessed_subjects_tms_fmri\")\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Make sure preprocessing_tms_fmri.py is importable\n",
        "# (place it in PROJECT_DIR or in the same folder as this notebook)\n",
        "if PROJECT_DIR not in sys.path:\n",
        "    sys.path.append(PROJECT_DIR)\n",
        "\n",
        "from preprocessing_tms_fmri import concat_runs, make_inputs_targets\n",
        "\n",
        "print(\"TMS pickle:\", TMS_PKL_PATH)\n",
        "print(\"Output dir:\", OUT_DIR)\n"
       ]
      },
      {
       "cell_type": "code",
       "execution_count": null,
       "id": "42c3804a",
       "metadata": {},
       "outputs": [],
       "source": [
        "# --- Parameters (match your HCP-style pipeline) ---\n",
        "# Window length (multi-to-one): input uses S past steps to predict next step\n",
        "USING_STEPS = 3          # S\n",
        "\n",
        "# Remove first N time points per run (to remove transient)\n",
        "REMOVE_POINTS = 30\n",
        "\n",
        "# Filtering band (same as HCP helper)\n",
        "LOW_HZ  = 0.008\n",
        "HIGH_HZ = 0.08\n",
        "ORDER   = 2\n",
        "\n",
        "# Standardization after filtering\n",
        "ZSCORE = True\n",
        "\n",
        "# TR handling:\n",
        "# - If TR_OVERRIDE is None: use per-run metadata['tr_s'] (recommended)\n",
        "# - If you want to force a TR (e.g., 2.4), set TR_OVERRIDE = 2.4\n",
        "TR_OVERRIDE = None\n",
        "\n",
        "DTYPE = np.float32\n",
        "\n",
        "print(\"USING_STEPS:\", USING_STEPS)\n",
        "print(\"REMOVE_POINTS:\", REMOVE_POINTS)\n",
        "print(\"Band (Hz):\", (LOW_HZ, HIGH_HZ), \"order\", ORDER)\n",
        "print(\"TR_OVERRIDE:\", TR_OVERRIDE)\n"
       ]
      },
      {
       "cell_type": "code",
       "execution_count": null,
       "id": "6a4c3ba5",
       "metadata": {},
       "outputs": [],
       "source": [
        "# --- Load dataset dictionary ---\n",
        "with open(TMS_PKL_PATH, \"rb\") as f:\n",
        "    dataset = pickle.load(f)\n",
        "\n",
        "print(\"Loaded subjects:\", len(dataset))\n",
        "# Quick peek\n",
        "some_sub = next(iter(dataset.keys()))\n",
        "print(\"Example subject:\", some_sub)\n",
        "print(\"Keys:\", list(dataset[some_sub].keys()))\n"
       ]
      },
      {
       "cell_type": "code",
       "execution_count": null,
       "id": "6fc13aa3",
       "metadata": {},
       "outputs": [],
       "source": [
        "# --- Build per-subject signals/inputs/targets for TASK-REST only ---\n",
        "saved_subjects = []\n",
        "summary = {}\n",
        "\n",
        "def get_rest_runs(sub_dict):\n",
        "    # sub_dict['task-rest'] is a dict keyed by int run_idx\n",
        "    if \"task-rest\" not in sub_dict:\n",
        "        return []\n",
        "    runs_dict = sub_dict[\"task-rest\"]\n",
        "    # sort by run_idx for determinism\n",
        "    return [runs_dict[k] for k in sorted(runs_dict.keys())]\n",
        "\n",
        "for sub_id, sub_dict in dataset.items():\n",
        "    rest_runs = get_rest_runs(sub_dict)\n",
        "    if not rest_runs:\n",
        "        continue\n",
        "\n",
        "    # Collect time series per run\n",
        "    run_ts = []\n",
        "    run_trs = []\n",
        "    run_sessions = []\n",
        "\n",
        "    for run_idx, run in sorted(sub_dict[\"task-rest\"].items()):\n",
        "        ts = run[\"time series\"]\n",
        "        md = run.get(\"metadata\", {})\n",
        "        tr = md.get(\"tr_s\", None)\n",
        "        ses = md.get(\"session\", None)\n",
        "\n",
        "        if ts is None or len(ts) == 0:\n",
        "            continue\n",
        "\n",
        "        if TR_OVERRIDE is not None:\n",
        "            tr = float(TR_OVERRIDE)\n",
        "        elif tr is None:\n",
        "            raise RuntimeError(f\"Missing tr_s in metadata for {sub_id} run {run_idx}\")\n",
        "\n",
        "        run_ts.append(ts)\n",
        "        run_trs.append(float(tr))\n",
        "        run_sessions.append(ses)\n",
        "\n",
        "    if not run_ts:\n",
        "        continue\n",
        "\n",
        "    # Ensure TR consistency across a subject's rest runs\n",
        "    # (If you ever have mixed TRs within 'task-rest', this will flag it.)\n",
        "    if len(set(np.round(run_trs, 6))) != 1:\n",
        "        raise RuntimeError(f\"Inconsistent TRs in {sub_id} task-rest runs: {run_trs}\")\n",
        "    tr_subject = run_trs[0]\n",
        "\n",
        "    # Preprocess + concatenate runs (filters each run separately by default)\n",
        "    signals = concat_runs(\n",
        "        run_ts,\n",
        "        tr=tr_subject,\n",
        "        n_drop=REMOVE_POINTS,\n",
        "        low=LOW_HZ,\n",
        "        high=HIGH_HZ,\n",
        "        order=ORDER,\n",
        "        zscore=ZSCORE,\n",
        "        filter_each_run=True,\n",
        "    ).astype(DTYPE, copy=False)\n",
        "\n",
        "    if signals.shape[0] <= USING_STEPS:\n",
        "        # too short after trimming\n",
        "        continue\n",
        "\n",
        "    inputs, targets = make_inputs_targets(signals, steps=USING_STEPS)\n",
        "    inputs = inputs.astype(DTYPE, copy=False)\n",
        "    targets = targets.astype(DTYPE, copy=False)\n",
        "\n",
        "    # Save (mirror HCP naming but with sub-... prefix)\n",
        "    sub_tag = sub_id  # keep \"sub-NTHC....\"\n",
        "    np.save(os.path.join(OUT_DIR, f\"{sub_tag}_signals.npy\"), signals)\n",
        "    np.save(os.path.join(OUT_DIR, f\"{sub_tag}_inputs.npy\"), inputs)\n",
        "    np.save(os.path.join(OUT_DIR, f\"{sub_tag}_targets.npy\"), targets)\n",
        "\n",
        "    saved_subjects.append(sub_tag)\n",
        "    summary[sub_tag] = {\n",
        "        \"n_runs\": len(run_ts),\n",
        "        \"sessions\": run_sessions,\n",
        "        \"tr_s\": tr_subject,\n",
        "        \"signals_shape\": list(signals.shape),\n",
        "        \"inputs_shape\": list(inputs.shape),\n",
        "        \"targets_shape\": list(targets.shape),\n",
        "    }\n",
        "\n",
        "print(\"Saved subjects:\", len(saved_subjects))\n",
        "print(\"Example summary:\", saved_subjects[0] if saved_subjects else None)\n"
       ]
      },
      {
       "cell_type": "code",
       "execution_count": null,
       "id": "94bda5e4",
       "metadata": {},
       "outputs": [],
       "source": [
        "# --- Save bookkeeping files (recommended) ---\n",
        "subjects_path = os.path.join(OUT_DIR, \"subjects_list_tms_fmri.txt\")\n",
        "with open(subjects_path, \"w\") as f:\n",
        "    for s in saved_subjects:\n",
        "        f.write(s + \"\\n\")\n",
        "\n",
        "summary_path = os.path.join(OUT_DIR, \"summary_tms_fmri_task_rest.json\")\n",
        "with open(summary_path, \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "config = {\n",
        "    \"dataset_pickle\": TMS_PKL_PATH,\n",
        "    \"task\": \"task-rest\",\n",
        "    \"atlas_order\": \"Tian50_then_Schaefer400\",\n",
        "    \"using_steps\": USING_STEPS,\n",
        "    \"remove_points_per_run\": REMOVE_POINTS,\n",
        "    \"band_hz\": [LOW_HZ, HIGH_HZ],\n",
        "    \"butter_order\": ORDER,\n",
        "    \"zscore\": ZSCORE,\n",
        "    \"tr_override\": TR_OVERRIDE,\n",
        "}\n",
        "config_path = os.path.join(OUT_DIR, \"preprocess_config_tms_fmri.json\")\n",
        "with open(config_path, \"w\") as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "print(\"Wrote:\", subjects_path)\n",
        "print(\"Wrote:\", summary_path)\n",
        "print(\"Wrote:\", config_path)\n"
       ]
      },
      {
       "cell_type": "markdown",
       "id": "2f9eaa41",
       "metadata": {},
       "source": [
        "### Understanding `signals`, `inputs`, and `targets`\n",
        "\n",
        "Let:\n",
        "\n",
        "- **T** = number of time points after trimming/filtering/concatenation (within subject)\n",
        "- **N** = number of parcels (here **450**)\n",
        "- **S** = `USING_STEPS`\n",
        "\n",
        "Then:\n",
        "\n",
        "- `signals` has shape **(T, N)**\n",
        "- `inputs` has shape **(T−S, S·N)** and contains flattened windows:  \n",
        "  `inputs[t] = signals[t : t+S].reshape(-1)`\n",
        "- `targets` has shape **(T−S, N)** and contains next-step targets:  \n",
        "  `targets[t] = signals[t+S]`\n"
       ]
      }
     ],
     "metadata": {
      "kernelspec": {
       "display_name": "Python 3 (ipykernel)",
       "language": "python",
       "name": "python3"
      },
      "language_info": {
       "codemirror_mode": {
        "name": "ipython",
        "version": 3
       },
       "file_extension": ".py",
       "mimetype": "text/x-python",
       "name": "python",
       "nbconvert_exporter": "python",
       "pygments_lexer": "ipython3",
       "version": "3.10.18"
      }
     },
     "nbformat": 4,
     "nbformat_minor": 5
    }
