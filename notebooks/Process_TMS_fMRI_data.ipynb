{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grabuffo/BrainStim_ANN_fMRI_HCP/blob/main/notebooks/Process_TMS_fMRI_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "386de11b-a574-4be4-9006-42c09b9bcfc7",
      "metadata": {
        "id": "386de11b-a574-4be4-9006-42c09b9bcfc7"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grabuffo/BrainStim_ANN_fMRI_HCP/blob/main/notebooks/Process_TMS_fMRI_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3dd4eba-0dd3-496b-bcb3-b715deddd230",
      "metadata": {
        "id": "e3dd4eba-0dd3-496b-bcb3-b715deddd230"
      },
      "source": [
        "# Process TMS-fMRI data (task-rest) for population ANN\n",
        "\n",
        "This notebook loads the preprocessed **TMS-fMRI** parcel time series stored in:\n",
        "\n",
        "- `data/TMS_fMRI/dataset_tian50_schaefer400_allruns.pkl`\n",
        "\n",
        "and creates, **per subject**, the arrays:\n",
        "\n",
        "- `*_signals.npy`  (T × 450)\n",
        "- `*_inputs.npy`   ((T−S) × (S·450))\n",
        "- `*_targets.npy`  ((T−S) × 450)\n",
        "\n",
        "where **S** is the number of past steps used as input (multi-to-one).\n",
        "\n",
        "Important design choices:\n",
        "- **No concatenation across subjects before building samples** (avoids artificial transitions).\n",
        "- Concatenation happens **within subject** across all available **task-rest** runs.\n",
        "- Filtering is applied **per run** before concatenation to avoid boundary artifacts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a7defc24",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7defc24",
        "outputId": "76293b9c-45a0-452c-ecdf-ed9ebb7d241c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Cloning into 'BrainStim_ANN_fMRI_HCP'...\n",
            "remote: Enumerating objects: 450, done.\u001b[K\n",
            "remote: Counting objects: 100% (100/100), done.\u001b[K\n",
            "remote: Compressing objects: 100% (90/90), done.\u001b[K\n",
            "remote: Total 450 (delta 45), reused 10 (delta 10), pack-reused 350 (from 2)\u001b[K\n",
            "Receiving objects: 100% (450/450), 62.69 MiB | 14.19 MiB/s, done.\n",
            "Resolving deltas: 100% (153/153), done.\n",
            "TMS pickle: /content/drive/MyDrive/Colab Notebooks/Brain_Stim_ANN/data/TMS_fMRI/dataset_tian50_schaefer400_allruns.pkl\n",
            "Output dir: /content/drive/MyDrive/Colab Notebooks/Brain_Stim_ANN/data/preprocessed_subjects_tms_fmri\n"
          ]
        }
      ],
      "source": [
        "# --- Setup ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import os, sys, json, pickle\n",
        "import numpy as np\n",
        "\n",
        "# Project + paths\n",
        "PROJECT_DIR = \"/content/drive/MyDrive/Colab Notebooks/Brain_Stim_ANN\"\n",
        "DATA_DIR    = os.path.join(PROJECT_DIR, \"data\")\n",
        "\n",
        "TMS_PKL_PATH = os.path.join(DATA_DIR, \"TMS_fMRI\", \"dataset_tian50_schaefer400_allruns.pkl\")\n",
        "OUT_DIR      = os.path.join(DATA_DIR, \"preprocessed_subjects_tms_fmri\")\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Make sure preprocessing_tms_fmri.py is importable\n",
        "# (place it in PROJECT_DIR or in the same folder as this notebook)\n",
        "if PROJECT_DIR not in sys.path:\n",
        "    sys.path.append(PROJECT_DIR)\n",
        "\n",
        "# 2 - Clone repo (only if missing)\n",
        "if not os.path.exists(\"/content/BrainStim_ANN_fMRI_HCP\"):\n",
        "    !git clone https://github.com/grabuffo/BrainStim_ANN_fMRI_HCP.git\n",
        "else:\n",
        "    print(\"Repo already exists ✅\")\n",
        "\n",
        "# 3 - Set paths\n",
        "repo_dir = \"/content/BrainStim_ANN_fMRI_HCP\"\n",
        "\n",
        "import sys\n",
        "sys.path.append(repo_dir)\n",
        "\n",
        "# 4 - Imports\n",
        "from src.preprocessing_tms_fmri import *\n",
        "from src.NPI import *\n",
        "\n",
        "#from preprocessing_tms_fmri import concat_runs, make_inputs_targets\n",
        "\n",
        "\n",
        "print(\"TMS pickle:\", TMS_PKL_PATH)\n",
        "print(\"Output dir:\", OUT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "42c3804a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42c3804a",
        "outputId": "8f1aac1a-4c08-42df-f55d-69f0ca1328f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "USING_STEPS: 3\n",
            "REMOVE_POINTS: 30\n",
            "Band (Hz): (0.008, 0.08) order 2\n",
            "TR_OVERRIDE: None\n"
          ]
        }
      ],
      "source": [
        "# --- Parameters (match your HCP-style pipeline) ---\n",
        "# Window length (multi-to-one): input uses S past steps to predict next step\n",
        "USING_STEPS = 3          # S\n",
        "\n",
        "# Remove first N time points per run (to remove transient)\n",
        "REMOVE_POINTS = 30\n",
        "\n",
        "# Filtering band (same as HCP helper)\n",
        "LOW_HZ  = 0.008\n",
        "HIGH_HZ = 0.08\n",
        "ORDER   = 2\n",
        "\n",
        "# Standardization after filtering\n",
        "ZSCORE = True\n",
        "\n",
        "# TR handling:\n",
        "# - If TR_OVERRIDE is None: use per-run metadata['tr_s'] (recommended)\n",
        "# - If you want to force a TR (e.g., 2.4), set TR_OVERRIDE = 2.4\n",
        "TR_OVERRIDE = None\n",
        "\n",
        "DTYPE = np.float32\n",
        "\n",
        "print(\"USING_STEPS:\", USING_STEPS)\n",
        "print(\"REMOVE_POINTS:\", REMOVE_POINTS)\n",
        "print(\"Band (Hz):\", (LOW_HZ, HIGH_HZ), \"order\", ORDER)\n",
        "print(\"TR_OVERRIDE:\", TR_OVERRIDE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6a4c3ba5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a4c3ba5",
        "outputId": "85f0f9db-daf8-4d71-ccbe-3c42f61adc95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-778669632.py:3: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
            "  dataset = pickle.load(f)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded subjects: 46\n",
            "Example subject: sub-NTHC1001\n",
            "Keys: ['task-rest', 'task-stim']\n"
          ]
        }
      ],
      "source": [
        "# --- Load dataset dictionary ---\n",
        "with open(TMS_PKL_PATH, \"rb\") as f:\n",
        "    dataset = pickle.load(f)\n",
        "\n",
        "print(\"Loaded subjects:\", len(dataset))\n",
        "# Quick peek\n",
        "some_sub = next(iter(dataset.keys()))\n",
        "print(\"Example subject:\", some_sub)\n",
        "print(\"Keys:\", list(dataset[some_sub].keys()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6fc13aa3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fc13aa3",
        "outputId": "f5ca795e-c5d2-428d-f67d-81438b3b8d0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved subjects: 46\n",
            "Example summary: sub-NTHC1001\n"
          ]
        }
      ],
      "source": [
        "# --- Build per-subject signals/inputs/targets for TASK-REST only ---\n",
        "saved_subjects = []\n",
        "summary = {}\n",
        "\n",
        "def get_rest_runs(sub_dict):\n",
        "    # sub_dict['task-rest'] is a dict keyed by int run_idx\n",
        "    if \"task-rest\" not in sub_dict:\n",
        "        return []\n",
        "    runs_dict = sub_dict[\"task-rest\"]\n",
        "    # sort by run_idx for determinism\n",
        "    return [runs_dict[k] for k in sorted(runs_dict.keys())]\n",
        "\n",
        "for sub_id, sub_dict in dataset.items():\n",
        "    rest_runs = get_rest_runs(sub_dict)\n",
        "    if not rest_runs:\n",
        "        continue\n",
        "\n",
        "    # Collect time series per run\n",
        "    run_ts = []\n",
        "    run_trs = []\n",
        "    run_sessions = []\n",
        "\n",
        "    for run_idx, run in sorted(sub_dict[\"task-rest\"].items()):\n",
        "        ts = run[\"time series\"]\n",
        "        md = run.get(\"metadata\", {})\n",
        "        tr = md.get(\"tr_s\", None)\n",
        "        ses = md.get(\"session\", None)\n",
        "\n",
        "        if ts is None or len(ts) == 0:\n",
        "            continue\n",
        "\n",
        "        if TR_OVERRIDE is not None:\n",
        "            tr = float(TR_OVERRIDE)\n",
        "        elif tr is None:\n",
        "            raise RuntimeError(f\"Missing tr_s in metadata for {sub_id} run {run_idx}\")\n",
        "\n",
        "        run_ts.append(ts)\n",
        "        run_trs.append(float(tr))\n",
        "        run_sessions.append(ses)\n",
        "\n",
        "    if not run_ts:\n",
        "        continue\n",
        "\n",
        "    # Ensure TR consistency across a subject's rest runs\n",
        "    # (If you ever have mixed TRs within 'task-rest', this will flag it.)\n",
        "    if len(set(np.round(run_trs, 6))) != 1:\n",
        "        raise RuntimeError(f\"Inconsistent TRs in {sub_id} task-rest runs: {run_trs}\")\n",
        "    tr_subject = run_trs[0]\n",
        "\n",
        "    # Preprocess + concatenate runs (filters each run separately by default)\n",
        "    signals = concat_runs(\n",
        "        run_ts,\n",
        "        tr=tr_subject,\n",
        "        n_drop=REMOVE_POINTS,\n",
        "        low=LOW_HZ,\n",
        "        high=HIGH_HZ,\n",
        "        order=ORDER,\n",
        "        zscore=ZSCORE,\n",
        "        filter_each_run=True,\n",
        "    ).astype(DTYPE, copy=False)\n",
        "\n",
        "    if signals.shape[0] <= USING_STEPS:\n",
        "        # too short after trimming\n",
        "        continue\n",
        "\n",
        "    inputs, targets = make_inputs_targets(signals, steps=USING_STEPS)\n",
        "    inputs = inputs.astype(DTYPE, copy=False)\n",
        "    targets = targets.astype(DTYPE, copy=False)\n",
        "\n",
        "    # Save (mirror HCP naming but with sub-... prefix)\n",
        "    sub_tag = sub_id  # keep \"sub-NTHC....\"\n",
        "    np.save(os.path.join(OUT_DIR, f\"{sub_tag}_signals.npy\"), signals)\n",
        "    np.save(os.path.join(OUT_DIR, f\"{sub_tag}_inputs.npy\"), inputs)\n",
        "    np.save(os.path.join(OUT_DIR, f\"{sub_tag}_targets.npy\"), targets)\n",
        "\n",
        "    saved_subjects.append(sub_tag)\n",
        "    summary[sub_tag] = {\n",
        "        \"n_runs\": len(run_ts),\n",
        "        \"sessions\": run_sessions,\n",
        "        \"tr_s\": tr_subject,\n",
        "        \"signals_shape\": list(signals.shape),\n",
        "        \"inputs_shape\": list(inputs.shape),\n",
        "        \"targets_shape\": list(targets.shape),\n",
        "    }\n",
        "\n",
        "print(\"Saved subjects:\", len(saved_subjects))\n",
        "print(\"Example summary:\", saved_subjects[0] if saved_subjects else None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "94bda5e4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94bda5e4",
        "outputId": "af53cfb6-e1a1-4356-d615-a3b732b60a18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote: /content/drive/MyDrive/Colab Notebooks/Brain_Stim_ANN/data/preprocessed_subjects_tms_fmri/subjects_list_tms_fmri.txt\n",
            "Wrote: /content/drive/MyDrive/Colab Notebooks/Brain_Stim_ANN/data/preprocessed_subjects_tms_fmri/summary_tms_fmri_task_rest.json\n",
            "Wrote: /content/drive/MyDrive/Colab Notebooks/Brain_Stim_ANN/data/preprocessed_subjects_tms_fmri/preprocess_config_tms_fmri.json\n"
          ]
        }
      ],
      "source": [
        "# --- Save bookkeeping files (recommended) ---\n",
        "subjects_path = os.path.join(OUT_DIR, \"subjects_list_tms_fmri.txt\")\n",
        "with open(subjects_path, \"w\") as f:\n",
        "    for s in saved_subjects:\n",
        "        f.write(s + \"\\n\")\n",
        "\n",
        "summary_path = os.path.join(OUT_DIR, \"summary_tms_fmri_task_rest.json\")\n",
        "with open(summary_path, \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "config = {\n",
        "    \"dataset_pickle\": TMS_PKL_PATH,\n",
        "    \"task\": \"task-rest\",\n",
        "    \"atlas_order\": \"Tian50_then_Schaefer400\",\n",
        "    \"using_steps\": USING_STEPS,\n",
        "    \"remove_points_per_run\": REMOVE_POINTS,\n",
        "    \"band_hz\": [LOW_HZ, HIGH_HZ],\n",
        "    \"butter_order\": ORDER,\n",
        "    \"zscore\": ZSCORE,\n",
        "    \"tr_override\": TR_OVERRIDE,\n",
        "}\n",
        "config_path = os.path.join(OUT_DIR, \"preprocess_config_tms_fmri.json\")\n",
        "with open(config_path, \"w\") as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "print(\"Wrote:\", subjects_path)\n",
        "print(\"Wrote:\", summary_path)\n",
        "print(\"Wrote:\", config_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f9eaa41",
      "metadata": {
        "id": "2f9eaa41"
      },
      "source": [
        "### Understanding `signals`, `inputs`, and `targets`\n",
        "\n",
        "Let:\n",
        "\n",
        "- **T** = number of time points after trimming/filtering/concatenation (within subject)\n",
        "- **N** = number of parcels (here **450**)\n",
        "- **S** = `USING_STEPS`\n",
        "\n",
        "Then:\n",
        "\n",
        "- `signals` has shape **(T, N)**\n",
        "- `inputs` has shape **(T−S, S·N)** and contains flattened windows:  \n",
        "  `inputs[t] = signals[t : t+S].reshape(-1)`\n",
        "- `targets` has shape **(T−S, N)** and contains next-step targets:  \n",
        "  `targets[t] = signals[t+S]`\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "huHMTezXrwdF"
      },
      "id": "huHMTezXrwdF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F5CSbIvkweMT"
      },
      "id": "F5CSbIvkweMT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UHBtvZOmweVp"
      },
      "id": "UHBtvZOmweVp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZzExucIOweaW"
      },
      "id": "ZzExucIOweaW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PnWDucCwwedb"
      },
      "id": "PnWDucCwwedb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zbsadUr8wefx"
      },
      "id": "zbsadUr8wefx",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Correctdness"
      ],
      "metadata": {
        "id": "L1uCgYAAwjPq"
      },
      "id": "L1uCgYAAwjPq"
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/Colab Notebooks/Brain_Stim_ANN/data/preprocessed_subjects_tms_fmri\"\n",
        "\n",
        "print(\"OUT_DIR exists:\", os.path.exists(OUT_DIR))\n",
        "print(\"npy files:\", len(glob.glob(os.path.join(OUT_DIR, \"*.npy\"))))\n",
        "print(\"json files:\", len(glob.glob(os.path.join(OUT_DIR, \"*.json\"))))\n",
        "print(\"txt files:\", len(glob.glob(os.path.join(OUT_DIR, \"*.txt\"))))\n",
        "\n",
        "print(\"\\nExample files:\")\n",
        "for f in sorted(glob.glob(os.path.join(OUT_DIR, \"*\")))[:10]:\n",
        "    print(\" \", os.path.basename(f))\n"
      ],
      "metadata": {
        "id": "CaW009fArwfp",
        "outputId": "62226a22-5776-49fd-c91d-22f5837e0133",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "CaW009fArwfp",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OUT_DIR exists: True\n",
            "npy files: 138\n",
            "json files: 2\n",
            "txt files: 1\n",
            "\n",
            "Example files:\n",
            "  ECts_MLP\n",
            "  preprocess_config_tms_fmri.json\n",
            "  sub-NTHC1001_inputs.npy\n",
            "  sub-NTHC1001_signals.npy\n",
            "  sub-NTHC1001_targets.npy\n",
            "  sub-NTHC1003_inputs.npy\n",
            "  sub-NTHC1003_signals.npy\n",
            "  sub-NTHC1003_targets.npy\n",
            "  sub-NTHC1009_inputs.npy\n",
            "  sub-NTHC1009_signals.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, glob, os\n",
        "\n",
        "sub = os.path.basename(sorted(glob.glob(os.path.join(OUT_DIR, \"sub-*_signals.npy\")))[0]).split(\"_signals.npy\")[0]\n",
        "signals = np.load(os.path.join(OUT_DIR, f\"{sub}_signals.npy\"))\n",
        "inputs  = np.load(os.path.join(OUT_DIR, f\"{sub}_inputs.npy\"))\n",
        "targets = np.load(os.path.join(OUT_DIR, f\"{sub}_targets.npy\"))\n",
        "\n",
        "print(\"Subject:\", sub)\n",
        "print(\"signals:\", signals.shape, signals.dtype)\n",
        "print(\"inputs :\", inputs.shape, inputs.dtype)\n",
        "print(\"targets:\", targets.shape, targets.dtype)\n",
        "\n",
        "# Basic integrity\n",
        "print(\"NaNs in signals:\", np.isnan(signals).any())\n",
        "print(\"NaNs in inputs :\", np.isnan(inputs).any())\n",
        "print(\"NaNs in targets:\", np.isnan(targets).any())\n",
        "\n",
        "# Dimension consistency\n",
        "T, N = signals.shape\n",
        "print(\"N regions (expected 450):\", N)\n",
        "print(\"targets rows == inputs rows:\", targets.shape[0] == inputs.shape[0])\n"
      ],
      "metadata": {
        "id": "om1jDbAerwi8",
        "outputId": "81455329-f665-45fb-fe2b-a77fc3d63f2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "om1jDbAerwi8",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subject: sub-NTHC1001\n",
            "signals: (210, 450) float32\n",
            "inputs : (207, 1350) float32\n",
            "targets: (207, 450) float32\n",
            "NaNs in signals: False\n",
            "NaNs in inputs : False\n",
            "NaNs in targets: False\n",
            "N regions (expected 450): 450\n",
            "targets rows == inputs rows: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Pick one parcel, compute FFT power ratio in/out of band\n",
        "tr = 2.4\n",
        "fs = 1.0 / tr\n",
        "x = signals[:, 100]  # any parcel\n",
        "\n",
        "freqs = np.fft.rfftfreq(len(x), d=tr)\n",
        "p = np.abs(np.fft.rfft(x))**2\n",
        "\n",
        "band = (freqs >= 0.008) & (freqs <= 0.08)\n",
        "ratio = p[band].sum() / (p.sum() + 1e-12)\n",
        "\n",
        "print(\"Bandpower ratio (0.008–0.08 Hz):\", float(ratio))\n"
      ],
      "metadata": {
        "id": "6h2oaRRTrwlN",
        "outputId": "1182a5fd-7fb3-4a53-b65c-c54c9438b35d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "6h2oaRRTrwlN",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bandpower ratio (0.008–0.08 Hz): 0.9618606567382812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "\n",
        "subs = sorted({os.path.basename(f).split(\"_signals.npy\")[0]\n",
        "               for f in glob.glob(os.path.join(OUT_DIR, \"sub-*_signals.npy\"))})\n",
        "\n",
        "missing = []\n",
        "for s in subs:\n",
        "    for suffix in [\"signals\", \"inputs\", \"targets\"]:\n",
        "        p = os.path.join(OUT_DIR, f\"{s}_{suffix}.npy\")\n",
        "        if not os.path.exists(p):\n",
        "            missing.append(p)\n",
        "\n",
        "print(\"n subjects:\", len(subs))\n",
        "print(\"missing files:\", len(missing))\n",
        "if missing[:10]:\n",
        "    print(\"First missing examples:\", missing[:10])\n"
      ],
      "metadata": {
        "id": "MgTJC28zrwn0",
        "outputId": "061b62d8-10da-40d2-81a9-44e50a1587f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "MgTJC28zrwn0",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n subjects: 46\n",
            "missing files: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, os\n",
        "\n",
        "summary = os.path.join(OUT_DIR, \"summary_tms_fmri_task_rest.json\")\n",
        "cfg     = os.path.join(OUT_DIR, \"preprocess_config_tms_fmri.json\")\n",
        "\n",
        "print(\"summary exists:\", os.path.exists(summary))\n",
        "print(\"config exists :\", os.path.exists(cfg))\n",
        "\n",
        "if os.path.exists(cfg):\n",
        "    with open(cfg, \"r\") as f:\n",
        "        print(\"\\nConfig:\", json.load(f))\n"
      ],
      "metadata": {
        "id": "qjrlBHYErwqJ",
        "outputId": "f8cefa9a-3069-4cc3-fc87-e6ee542917de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "qjrlBHYErwqJ",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "summary exists: True\n",
            "config exists : True\n",
            "\n",
            "Config: {'dataset_pickle': '/content/drive/MyDrive/Colab Notebooks/Brain_Stim_ANN/data/TMS_fMRI/dataset_tian50_schaefer400_allruns.pkl', 'task': 'task-rest', 'atlas_order': 'Tian50_then_Schaefer400', 'using_steps': 3, 'remove_points_per_run': 30, 'band_hz': [0.008, 0.08], 'butter_order': 2, 'zscore': True, 'tr_override': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "GhAoFpa3rwsX"
      },
      "id": "GhAoFpa3rwsX",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/Colab Notebooks/Brain_Stim_ANN/data/preprocessed_subjects_tms_fmri\"\n",
        "cfg_path = os.path.join(OUT_DIR, \"preprocess_config_tms_fmri.json\")\n",
        "\n",
        "with open(cfg_path, \"r\") as f:\n",
        "    cfg = json.load(f)\n",
        "\n",
        "cfg\n"
      ],
      "metadata": {
        "id": "nlF19BvNrwwd",
        "outputId": "70f420d0-67c0-42f9-e580-3f77fb12221f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "nlF19BvNrwwd",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dataset_pickle': '/content/drive/MyDrive/Colab Notebooks/Brain_Stim_ANN/data/TMS_fMRI/dataset_tian50_schaefer400_allruns.pkl',\n",
              " 'task': 'task-rest',\n",
              " 'atlas_order': 'Tian50_then_Schaefer400',\n",
              " 'using_steps': 3,\n",
              " 'remove_points_per_run': 30,\n",
              " 'band_hz': [0.008, 0.08],\n",
              " 'butter_order': 2,\n",
              " 'zscore': True,\n",
              " 'tr_override': None}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pickle, numpy as np\n",
        "from src.preprocessing_tms_fmri import concat_runs\n",
        "\n",
        "PICKLE_PATH = \"/content/drive/MyDrive/Colab Notebooks/Brain_Stim_ANN/data/TMS_fMRI/dataset_tian50_schaefer400_allruns.pkl\"\n",
        "OUT_DIR     = \"/content/drive/MyDrive/Colab Notebooks/Brain_Stim_ANN/data/preprocessed_subjects_tms_fmri\"\n",
        "sub_id      = \"sub-NTHC1001\"\n",
        "\n",
        "saved = np.load(os.path.join(OUT_DIR, f\"{sub_id}_signals.npy\"))\n",
        "\n",
        "with open(PICKLE_PATH, \"rb\") as f:\n",
        "    dataset = pickle.load(f)\n",
        "\n",
        "rest_runs = [dataset[sub_id][\"task-rest\"][k][\"time series\"] for k in sorted(dataset[sub_id][\"task-rest\"].keys())]\n",
        "\n",
        "def score(arr):\n",
        "    diff = saved - arr\n",
        "    return float(np.max(np.abs(diff))), float(np.mean(np.abs(diff)))\n",
        "\n",
        "candidates = []\n",
        "for filter_each_run in [True, False]:\n",
        "    for zscore in [True, False]:\n",
        "        for n_drop in [30, 0]:\n",
        "            rec = concat_runs(\n",
        "                rest_runs,\n",
        "                tr=2., n_drop=n_drop,\n",
        "                low=0.008, high=0.08, order=2,\n",
        "                zscore=zscore,\n",
        "                filter_each_run=filter_each_run\n",
        "            )\n",
        "            if rec.shape != saved.shape:\n",
        "                continue\n",
        "            mx, me = score(rec)\n",
        "            candidates.append((mx, me, filter_each_run, zscore, n_drop))\n",
        "\n",
        "candidates = sorted(candidates, key=lambda x: (x[0], x[1]))\n",
        "print(\"Best matches (max_abs, mean_abs, filter_each_run, zscore, n_drop):\")\n",
        "for row in candidates[:5]:\n",
        "    print(row)\n"
      ],
      "metadata": {
        "id": "4JR6whtyvaI6",
        "outputId": "ec7827d2-647a-48bd-f207-7c4ba8e7af40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "4JR6whtyvaI6",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2385822940.py:11: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
            "  dataset = pickle.load(f)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best matches (max_abs, mean_abs, filter_each_run, zscore, n_drop):\n",
            "(0.0, 0.0, True, True, 30)\n",
            "(0.0, 0.0, False, True, 30)\n",
            "(2.589588165283203, 0.28962451219558716, True, False, 30)\n",
            "(2.589588165283203, 0.28962451219558716, False, False, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pickle, numpy as np\n",
        "from src.preprocessing_tms_fmri import concat_runs\n",
        "\n",
        "PICKLE_PATH = \"/content/drive/MyDrive/Colab Notebooks/Brain_Stim_ANN/data/TMS_fMRI/dataset_tian50_schaefer400_allruns.pkl\"\n",
        "OUT_DIR     = \"/content/drive/MyDrive/Colab Notebooks/Brain_Stim_ANN/data/preprocessed_subjects_tms_fmri\"\n",
        "sub_id      = \"sub-NTHC1001\"\n",
        "\n",
        "saved = np.load(os.path.join(OUT_DIR, f\"{sub_id}_signals.npy\"))\n",
        "\n",
        "with open(PICKLE_PATH, \"rb\") as f:\n",
        "    dataset = pickle.load(f)\n",
        "\n",
        "rest_runs = [dataset[sub_id][\"task-rest\"][k][\"time series\"] for k in sorted(dataset[sub_id][\"task-rest\"].keys())]\n",
        "\n",
        "# IMPORTANT: rest TR = 2.0s\n",
        "recomputed = concat_runs(\n",
        "    rest_runs,\n",
        "    tr=2.0,\n",
        "    n_drop=30,\n",
        "    low=0.008, high=0.08, order=2,\n",
        "    zscore=True,\n",
        "    filter_each_run=True\n",
        ")\n",
        "\n",
        "print(\"Saved shape      :\", saved.shape)\n",
        "print(\"Recomputed shape :\", recomputed.shape)\n",
        "\n",
        "diff = saved - recomputed\n",
        "print(\"Max abs diff :\", float(np.max(np.abs(diff))))\n",
        "print(\"Mean abs diff:\", float(np.mean(np.abs(diff))))\n",
        "print(\"Allclose?    :\", np.allclose(saved, recomputed, atol=1e-5, rtol=1e-5))\n"
      ],
      "metadata": {
        "id": "hBjTZGi-vd_x",
        "outputId": "a94b8126-dc65-40a5-b954-c347b81e707b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "hBjTZGi-vd_x",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1665719293.py:11: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
            "  dataset = pickle.load(f)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved shape      : (210, 450)\n",
            "Recomputed shape : (210, 450)\n",
            "Max abs diff : 0.0\n",
            "Mean abs diff: 0.0\n",
            "Allclose?    : True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VOc2dYc_wZNV"
      },
      "id": "VOc2dYc_wZNV",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}