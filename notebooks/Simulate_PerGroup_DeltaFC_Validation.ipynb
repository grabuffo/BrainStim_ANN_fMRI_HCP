{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "329e1bf6",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/grabuffo/BrainStim_ANN_fMRI_HCP/blob/main/notebooks/Simulate_PerGroup_DeltaFC_Validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc9319f",
   "metadata": {},
   "source": [
    "# Simulate Per-Group Models & Validate with Subject-Specific Î”FC\n",
    "\n",
    "Generate synthetic TMS-fMRI sessions using target-specific models trained per-group.\n",
    "For each subject, generate:\n",
    "- Rest sessions: one per stim session (matched pairs)\n",
    "- Stim sessions: using their target-specific model with real stimulation timing\n",
    "\n",
    "Then validate by comparing empirical vs. simulated Î”FC = FC_stim - FC_rest per subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e53c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 0) Mount Google Drive\n",
    "# =========================\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39977e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1) Clone repo + imports\n",
    "# =========================\n",
    "import os, sys, pickle, json, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "REPO_DIR = \"/content/BrainStim_ANN_fMRI_HCP\"\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone https://github.com/grabuffo/BrainStim_ANN_fMRI_HCP.git\n",
    "else:\n",
    "    print(\"Repo already exists âœ…\")\n",
    "\n",
    "sys.path.append(REPO_DIR)\n",
    "\n",
    "from src.NPI import build_model, device\n",
    "print(\"Torch device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe4c3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 2) Paths (EDIT IF NEEDED)\n",
    "# =========================\n",
    "BASE = \"/content/drive/MyDrive/Colab Notebooks/Brain_Stim_ANN/data\"\n",
    "\n",
    "DATASET_EMP_PKL = os.path.join(BASE, \"TMS_fMRI\", \"dataset_tian50_schaefer400_allruns.pkl\")\n",
    "GROUPING_SUMMARY_PKL = os.path.join(BASE, \"TMS_fMRI\", \"target_regions_grouping_summary.pkl\")\n",
    "\n",
    "PREPROC_ROOT = os.path.join(BASE, \"preprocessed_subjects_tms_fmri\")\n",
    "MODELS_DIR = os.path.join(PREPROC_ROOT, \"trained_models_MLP_tms_fmri_pergroup\")\n",
    "\n",
    "OUT_DIR = os.path.join(PREPROC_ROOT, \"ANN_vs_tms_fmri_pergroup\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "OUT_PKL = os.path.join(OUT_DIR, \"dataset_simulated_pergroup_ANN.pkl\")\n",
    "\n",
    "print(\"Empirical dataset:\", DATASET_EMP_PKL, \"| exists:\", os.path.exists(DATASET_EMP_PKL))\n",
    "print(\"Grouping summary:\", GROUPING_SUMMARY_PKL, \"| exists:\", os.path.exists(GROUPING_SUMMARY_PKL))\n",
    "print(\"Models directory:\", MODELS_DIR, \"| exists:\", os.path.exists(MODELS_DIR))\n",
    "print(\"Will save to:\", OUT_PKL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0f2fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 3) Load empirical dataset + grouping summary\n",
    "# =========================\n",
    "with open(DATASET_EMP_PKL, \"rb\") as f:\n",
    "    dataset_emp = pickle.load(f)\n",
    "\n",
    "with open(GROUPING_SUMMARY_PKL, \"rb\") as f:\n",
    "    grouping_summary = pickle.load(f)\n",
    "\n",
    "subjects_list = grouping_summary['subjects_list']\n",
    "multi_subject_groups = grouping_summary['multi_subject_groups']\n",
    "subject_targets_dict = grouping_summary['subject_targets']\n",
    "\n",
    "print(f\"Loaded {len(dataset_emp)} subjects\")\n",
    "print(f\"Grouping: {len(multi_subject_groups)} multi-subject groups\")\n",
    "\n",
    "# Build mapping: subject_id -> target_id (from their stim sessions)\n",
    "subject_to_target = {}\n",
    "for target_id, group_info in multi_subject_groups.items():\n",
    "    for sid in group_info['subject_ids']:\n",
    "        subject_to_target[sid] = target_id\n",
    "\n",
    "print(f\"\\nSubject to target mapping:\")\n",
    "for sid in range(len(subjects_list)):\n",
    "    if sid in subject_to_target:\n",
    "        print(f\"  Subject {sid} ({subjects_list[sid]}): target {subject_to_target[sid]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2611525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 4) Config\n",
    "# =========================\n",
    "S = 3\n",
    "N = 450\n",
    "\n",
    "TR_MODEL = 2.0          # model step in seconds\n",
    "BURN_IN = 10           # steps (not saved)\n",
    "NOISE_SIGMA = 0.3      # z-scored units\n",
    "STIM_AMP = 1.0        # z-scored units\n",
    "\n",
    "MAP_MODE = \"round\"      # \"round\" | \"floor\" | \"ceil\" for mapping onset_s -> model steps\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "# Distance matrix + stimulation kernel\n",
    "RHO_MM = 10.0\n",
    "DIST_PATH = os.path.join(BASE, \"TMS_fMRI\", \"atlases\", \"distance_matrix_450x450_Tian50_Schaefer400.npy\")\n",
    "D = np.load(DIST_PATH)\n",
    "assert D.shape == (N, N)\n",
    "\n",
    "# Gaussian weights\n",
    "W = np.exp(-(D ** 2) / (2.0 * (RHO_MM ** 2))).astype(np.float32)\n",
    "W /= (W[np.arange(N), np.arange(N)][:, None] + 1e-8)\n",
    "\n",
    "print(f\"Config: TR={TR_MODEL}s, S={S}, N={N}\")\n",
    "print(f\"Distance matrix: {D.min():.1f}-{D.max():.1f} mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d541c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 5) Helpers\n",
    "# =========================\n",
    "def get_onset_column(df: pd.DataFrame):\n",
    "    if df is None or len(df) == 0:\n",
    "        return None\n",
    "    for c in [\"onset\", \"Onset\", \"stim_onset\", \"onset_s\", \"onset_sec\", \"time\", \"t\", \"seconds\"]:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    for c in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[c]):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def map_onsets_to_steps(onsets_s, tr_model=TR_MODEL, mode=MAP_MODE):\n",
    "    onsets_s = np.asarray(onsets_s, dtype=float)\n",
    "    x = onsets_s / float(tr_model)\n",
    "    if mode == \"round\":\n",
    "        steps = np.rint(x).astype(int)\n",
    "    elif mode == \"floor\":\n",
    "        steps = np.floor(x).astype(int)\n",
    "    elif mode == \"ceil\":\n",
    "        steps = np.ceil(x).astype(int)\n",
    "    else:\n",
    "        raise ValueError(\"mode must be round|floor|ceil\")\n",
    "    steps = steps[steps >= 0]\n",
    "    return np.unique(steps)\n",
    "\n",
    "def safe_target_idx(target_vec):\n",
    "    if target_vec is None:\n",
    "        return None\n",
    "    v = np.asarray(target_vec).astype(int).ravel()\n",
    "    if v.size == 0 or v.sum() != 1:\n",
    "        return None\n",
    "    return int(np.argmax(v))\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_next(model, window_SxN: np.ndarray):\n",
    "    x_np = window_SxN.reshape(-1).astype(np.float32)\n",
    "    noise = NOISE_SIGMA * rng.normal(0.0, 1.0, size=x_np.shape).astype(np.float32)\n",
    "    x_np = x_np + noise\n",
    "    x = torch.tensor(x_np[None, :], dtype=torch.float32, device=device)\n",
    "    y = model(x)\n",
    "    return y.detach().cpu().numpy().squeeze(0)\n",
    "\n",
    "def simulate_run(model, init_window_SxN, n_steps,\n",
    "                 stim_steps=None, target_idx=None,\n",
    "                 stim_amp=STIM_AMP,\n",
    "                 burn_in=BURN_IN, W=None):\n",
    "    init_window_SxN = np.asarray(init_window_SxN, dtype=np.float32)\n",
    "    assert init_window_SxN.shape == (S, N)\n",
    "    stim_steps = set(int(s) for s in (stim_steps or []))\n",
    "    do_stim = (target_idx is not None) and (len(stim_steps) > 0)\n",
    "    w = init_window_SxN.copy()\n",
    "\n",
    "    # Burn-in\n",
    "    for _ in range(burn_in):\n",
    "        y = predict_next(model, w)\n",
    "        w = np.vstack([w[1:], y[None, :]])\n",
    "\n",
    "    # Session\n",
    "    out = np.zeros((n_steps, N), dtype=np.float32)\n",
    "    for t in range(n_steps):\n",
    "        w_in = w.copy()\n",
    "        if do_stim and (t in stim_steps):\n",
    "            if W is None:\n",
    "                w_in[-1, target_idx] += stim_amp\n",
    "            else:\n",
    "                w_in[-1, :] += stim_amp * W[target_idx, :]\n",
    "        y = predict_next(model, w_in)\n",
    "        out[t] = y\n",
    "        w = np.vstack([w[1:], y[None, :]])\n",
    "\n",
    "    meta_sim = {\n",
    "        \"tr_model_s\": float(TR_MODEL),\n",
    "        \"burn_in_steps\": int(burn_in),\n",
    "        \"noise_input_sigma\": 0.3,\n",
    "        \"stim_amp\": float(stim_amp),\n",
    "        \"stim_steps_modelTR\": sorted(list(stim_steps)) if do_stim else [],\n",
    "        \"stim_mapping_mode\": MAP_MODE,\n",
    "    }\n",
    "    return out, meta_sim\n",
    "\n",
    "print(\"âœ… Helpers defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77f4b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 6) Load per-group models\n",
    "# =========================\n",
    "models_cache = {}\n",
    "\n",
    "for target_id in sorted(multi_subject_groups.keys()):\n",
    "    model_path = os.path.join(MODELS_DIR, f\"target_{target_id:03d}_MLP_tms_fmri.pt\")\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model not found: {model_path}\")\n",
    "    \n",
    "    try:\n",
    "        state = torch.load(model_path, map_location=device, weights_only=True)\n",
    "        model = build_model(\"MLP\", ROI_num=N, using_steps=S).to(device)\n",
    "        if isinstance(state, dict) and \"state_dict\" in state:\n",
    "            model.load_state_dict(state[\"state_dict\"])\n",
    "        elif isinstance(state, dict):\n",
    "            model.load_state_dict(state)\n",
    "        else:\n",
    "            raise RuntimeError(\"Unexpected format\")\n",
    "        print(f\"Loaded weights with weights_only=True for target {target_id}\")\n",
    "    except Exception as e:\n",
    "        state = torch.load(model_path, map_location=device, weights_only=False)\n",
    "        model = build_model(\"MLP\", ROI_num=N, using_steps=S).to(device)\n",
    "        if isinstance(state, dict) and \"state_dict\" in state:\n",
    "            model.load_state_dict(state[\"state_dict\"])\n",
    "        elif isinstance(state, dict):\n",
    "            model.load_state_dict(state)\n",
    "        else:\n",
    "            model = state.to(device)\n",
    "        print(f\"Loaded weights with weights_only=False for target {target_id}\")\n",
    "    \n",
    "    model.eval()\n",
    "    models_cache[target_id] = model\n",
    "\n",
    "print(f\"\\nâœ… Loaded {len(models_cache)} per-group models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d0e191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 7) Build dataset_simulated with per-group models\n",
    "# =========================\n",
    "dataset_sim = {}\n",
    "n_sim_rest = 0\n",
    "n_sim_stim = 0\n",
    "\n",
    "for sub_id, sub_data in dataset_emp.items():\n",
    "    dataset_sim[sub_id] = {\"task-rest\": {}, \"task-stim\": {}}\n",
    "    \n",
    "    # Extract subject index\n",
    "    sub_name = sub_id.replace(\"sub-\", \"\")\n",
    "    subject_idx = subjects_list.index(sub_name) if sub_name in subjects_list else None\n",
    "    \n",
    "    if subject_idx is None or subject_idx not in subject_to_target:\n",
    "        print(f\"âš ï¸ {sub_id}: not in multi-subject groups, skipping\")\n",
    "        continue\n",
    "    \n",
    "    target_id = subject_to_target[subject_idx]\n",
    "    model = models_cache[target_id]\n",
    "    \n",
    "    print(f\"\\n{sub_id}: target={target_id}, model loaded\")\n",
    "    \n",
    "    # Get all stim runs for this subject\n",
    "    stim_runs = sub_data.get(\"task-stim\", {})\n",
    "    n_stim_runs = len(stim_runs)\n",
    "    \n",
    "    # Generate REST sessions (one per stim session)\n",
    "    for sim_rest_idx in range(n_stim_runs):\n",
    "        # Use first empirical rest as template for duration\n",
    "        emp_rest_template = None\n",
    "        if \"task-rest\" in sub_data and len(sub_data[\"task-rest\"]) > 0:\n",
    "            emp_rest_template = list(sub_data[\"task-rest\"].values())[0]\n",
    "        \n",
    "        if emp_rest_template is None:\n",
    "            print(f\"  âš ï¸  No empirical rest template, skipping rest simulations\")\n",
    "            break\n",
    "        \n",
    "        ts_emp = emp_rest_template.get(\"time series\", None)\n",
    "        md_emp = emp_rest_template.get(\"metadata\", {}) or {}\n",
    "        \n",
    "        if ts_emp is None or not isinstance(ts_emp, np.ndarray) or ts_emp.shape[1] != N:\n",
    "            continue\n",
    "        \n",
    "        tr_emp = float(md_emp.get(\"tr_s\", 2.0))\n",
    "        dur_s = ts_emp.shape[0] * tr_emp\n",
    "        n_steps = int(math.ceil(dur_s / TR_MODEL))\n",
    "        \n",
    "        init_window = ts_emp[:S].copy()\n",
    "        sim_ts, meta_sim = simulate_run(model, init_window, n_steps)\n",
    "        \n",
    "        md_out = dict(md_emp)\n",
    "        md_out.update({\n",
    "            \"simulated\": True,\n",
    "            \"source_empirical_task\": \"task-rest\",\n",
    "            \"target_id\": int(target_id),\n",
    "            \"duration_emp_s\": float(dur_s),\n",
    "            \"n_steps_model\": int(n_steps),\n",
    "            **meta_sim\n",
    "        })\n",
    "        \n",
    "        dataset_sim[sub_id][\"task-rest\"][int(sim_rest_idx)] = {\n",
    "            \"time series\": sim_ts,\n",
    "            \"metadata\": md_out\n",
    "        }\n",
    "        n_sim_rest += 1\n",
    "    \n",
    "    # Generate STIM sessions\n",
    "    for stim_run_idx, stim_run in stim_runs.items():\n",
    "        ts_emp = stim_run.get(\"time series\", None)\n",
    "        md_emp = stim_run.get(\"metadata\", {}) or {}\n",
    "        target_vec = stim_run.get(\"target\", None)\n",
    "        events_df = stim_run.get(\"stim time\", None)\n",
    "        \n",
    "        if ts_emp is None or not isinstance(ts_emp, np.ndarray) or ts_emp.shape[1] != N:\n",
    "            continue\n",
    "        \n",
    "        target_idx_emp = safe_target_idx(target_vec)\n",
    "        if target_idx_emp is None:\n",
    "            continue\n",
    "        \n",
    "        onset_col = get_onset_column(events_df) if isinstance(events_df, pd.DataFrame) else None\n",
    "        if onset_col is None:\n",
    "            continue\n",
    "        \n",
    "        onsets_s = events_df[onset_col].astype(float).values\n",
    "        stim_steps = list(map_onsets_to_steps(onsets_s, tr_model=TR_MODEL, mode=MAP_MODE))\n",
    "        \n",
    "        tr_emp = float(md_emp.get(\"tr_s\", 2.4))\n",
    "        dur_s = ts_emp.shape[0] * tr_emp\n",
    "        n_steps = int(math.ceil(dur_s / TR_MODEL))\n",
    "        \n",
    "        init_window = ts_emp[:S].copy()\n",
    "        sim_ts, meta_sim = simulate_run(model, init_window, n_steps,\n",
    "                                        stim_steps=stim_steps, target_idx=target_idx_emp, W=W)\n",
    "        \n",
    "        md_out = dict(md_emp)\n",
    "        md_out.update({\n",
    "            \"simulated\": True,\n",
    "            \"source_empirical_task\": \"task-stim\",\n",
    "            \"target_id\": int(target_id),\n",
    "            \"duration_emp_s\": float(dur_s),\n",
    "            \"n_steps_model\": int(n_steps),\n",
    "            \"target_idx\": int(target_idx_emp),\n",
    "            **meta_sim\n",
    "        })\n",
    "        \n",
    "        dataset_sim[sub_id][\"task-stim\"][int(stim_run_idx)] = {\n",
    "            \"time series\": sim_ts,\n",
    "            \"metadata\": md_out,\n",
    "            \"target\": target_vec,\n",
    "            \"stim time\": events_df,\n",
    "        }\n",
    "        n_sim_stim += 1\n",
    "\n",
    "print(f\"\\nâœ… Simulated {n_sim_rest} rest runs, {n_sim_stim} stim runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf62dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 8) Save to Drive\n",
    "# =========================\n",
    "with open(OUT_PKL, \"wb\") as f:\n",
    "    pickle.dump(dataset_sim, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(f\"Saved dataset_simulated to: {OUT_PKL}\")\n",
    "\n",
    "# Quick peek\n",
    "some_sub = next(iter(dataset_sim.keys()), None)\n",
    "if some_sub:\n",
    "    print(f\"\\nExample subject: {some_sub}\")\n",
    "    print(f\"task-rest runs: {list(dataset_sim[some_sub]['task-rest'].keys())[:5]}\")\n",
    "    print(f\"task-stim runs: {list(dataset_sim[some_sub]['task-stim'].keys())[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd88988",
   "metadata": {},
   "source": [
    "# Subject-Specific Î”FC Validation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0020ea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 9) Load data for validation\n",
    "# =========================\n",
    "print(\"Loading empirical dataset...\")\n",
    "with open(DATASET_EMP_PKL, \"rb\") as f:\n",
    "    dataset_emp = pickle.load(f)\n",
    "\n",
    "print(\"Loading simulated dataset...\")\n",
    "with open(OUT_PKL, \"rb\") as f:\n",
    "    dataset_sim = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded {len(dataset_emp)} empirical subjects\")\n",
    "print(f\"Loaded {len(dataset_sim)} simulated subjects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884952b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 10) Define validation functions\n",
    "# =========================\n",
    "def fc_from_timeseries(ts, cortical_only=False):\n",
    "    \"\"\"Compute Pearson FC matrix from time series.\"\"\"\n",
    "    if cortical_only:\n",
    "        ts = ts[:, 50:]  # Skip Tian 50, keep Schaefer 400\n",
    "    return np.corrcoef(ts, rowvar=False).astype(np.float32)\n",
    "\n",
    "def upper_tri_vec(mat, k=1):\n",
    "    \"\"\"Extract upper triangle as 1D vector.\"\"\"\n",
    "    iu = np.triu_indices(mat.shape[0], k=k)\n",
    "    return mat[iu]\n",
    "\n",
    "def pearson_corr(x, y):\n",
    "    \"\"\"Compute Pearson correlation.\"\"\"\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    y = np.asarray(y, dtype=np.float64)\n",
    "    x = x - x.mean()\n",
    "    y = y - y.mean()\n",
    "    denom = np.sqrt(np.sum(x*x) * np.sum(y*y))\n",
    "    return np.nan if denom == 0 else float(np.sum(x*y) / denom)\n",
    "\n",
    "print(\"âœ… Validation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55173de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 11) Compute per-subject Î”FC and correlations\n",
    "# =========================\n",
    "results = {\n",
    "    'subject_deltafc_corr': {},  # sub_id -> correlation\n",
    "    'subject_deltafc_vec_emp': {},  # sub_id -> vector\n",
    "    'subject_deltafc_vec_sim': {},  # sub_id -> vector\n",
    "}\n",
    "\n",
    "for sub_id in sorted(dataset_emp.keys()):\n",
    "    # Check if we have both empirical and simulated data\n",
    "    if sub_id not in dataset_sim:\n",
    "        print(f\"âš ï¸  {sub_id}: no simulated data, skipping\")\n",
    "        continue\n",
    "    \n",
    "    sub_emp = dataset_emp[sub_id]\n",
    "    sub_sim = dataset_sim[sub_id]\n",
    "    \n",
    "    # Get rest and stim sessions\n",
    "    rest_runs_emp = sub_emp.get(\"task-rest\", {})\n",
    "    stim_runs_emp = sub_emp.get(\"task-stim\", {})\n",
    "    rest_runs_sim = sub_sim.get(\"task-rest\", {})\n",
    "    stim_runs_sim = sub_sim.get(\"task-stim\", {})\n",
    "    \n",
    "    if len(rest_runs_emp) == 0 or len(stim_runs_emp) == 0:\n",
    "        print(f\"âš ï¸  {sub_id}: missing rest or stim sessions, skipping\")\n",
    "        continue\n",
    "    \n",
    "    if len(rest_runs_sim) == 0 or len(stim_runs_sim) == 0:\n",
    "        print(f\"âš ï¸  {sub_id}: missing simulated sessions, skipping\")\n",
    "        continue\n",
    "    \n",
    "    # Average rest FC (empirical and simulated)\n",
    "    fc_rest_emp_list = []\n",
    "    for run in rest_runs_emp.values():\n",
    "        ts = run.get(\"time series\", None)\n",
    "        if isinstance(ts, np.ndarray) and ts.shape[1] >= 450:\n",
    "            fc_rest_emp_list.append(fc_from_timeseries(ts, cortical_only=True))\n",
    "    \n",
    "    fc_rest_sim_list = []\n",
    "    for run in rest_runs_sim.values():\n",
    "        ts = run.get(\"time series\", None)\n",
    "        if isinstance(ts, np.ndarray) and ts.shape[1] >= 450:\n",
    "            fc_rest_sim_list.append(fc_from_timeseries(ts, cortical_only=True))\n",
    "    \n",
    "    if len(fc_rest_emp_list) == 0 or len(fc_rest_sim_list) == 0:\n",
    "        print(f\"âš ï¸  {sub_id}: no valid rest sessions, skipping\")\n",
    "        continue\n",
    "    \n",
    "    FC_rest_emp = np.mean(np.stack(fc_rest_emp_list), axis=0)\n",
    "    FC_rest_sim = np.mean(np.stack(fc_rest_sim_list), axis=0)\n",
    "    \n",
    "    # Average stim FC\n",
    "    fc_stim_emp_list = []\n",
    "    for run in stim_runs_emp.values():\n",
    "        ts = run.get(\"time series\", None)\n",
    "        if isinstance(ts, np.ndarray) and ts.shape[1] >= 450:\n",
    "            fc_stim_emp_list.append(fc_from_timeseries(ts, cortical_only=True))\n",
    "    \n",
    "    fc_stim_sim_list = []\n",
    "    for run in stim_runs_sim.values():\n",
    "        ts = run.get(\"time series\", None)\n",
    "        if isinstance(ts, np.ndarray) and ts.shape[1] >= 450:\n",
    "            fc_stim_sim_list.append(fc_from_timeseries(ts, cortical_only=True))\n",
    "    \n",
    "    if len(fc_stim_emp_list) == 0 or len(fc_stim_sim_list) == 0:\n",
    "        print(f\"âš ï¸  {sub_id}: no valid stim sessions, skipping\")\n",
    "        continue\n",
    "    \n",
    "    FC_stim_emp = np.mean(np.stack(fc_stim_emp_list), axis=0)\n",
    "    FC_stim_sim = np.mean(np.stack(fc_stim_sim_list), axis=0)\n",
    "    \n",
    "    # Compute Î”FC\n",
    "    deltaFC_emp = FC_stim_emp - FC_rest_emp\n",
    "    deltaFC_sim = FC_stim_sim - FC_rest_sim\n",
    "    \n",
    "    # Extract upper triangle\n",
    "    vec_emp = upper_tri_vec(deltaFC_emp, k=1)\n",
    "    vec_sim = upper_tri_vec(deltaFC_sim, k=1)\n",
    "    \n",
    "    # Correlate\n",
    "    r = pearson_corr(vec_emp, vec_sim)\n",
    "    \n",
    "    results['subject_deltafc_corr'][sub_id] = r\n",
    "    results['subject_deltafc_vec_emp'][sub_id] = vec_emp\n",
    "    results['subject_deltafc_vec_sim'][sub_id] = vec_sim\n",
    "    \n",
    "    print(f\"{sub_id}: r_Î”FC = {r:.4f}\")\n",
    "\n",
    "print(f\"\\nâœ… Computed per-subject Î”FC correlations for {len(results['subject_deltafc_corr'])} subjects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ac5222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 12) Summary statistics\n",
    "# =========================\n",
    "corrs = np.array(list(results['subject_deltafc_corr'].values()))\n",
    "corrs_valid = corrs[np.isfinite(corrs)]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PER-SUBJECT Î”FC CORRELATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"N subjects: {len(corrs_valid)}\")\n",
    "print(f\"Mean r: {corrs_valid.mean():.4f}\")\n",
    "print(f\"Std r: {corrs_valid.std():.4f}\")\n",
    "print(f\"Median r: {np.median(corrs_valid):.4f}\")\n",
    "print(f\"Min r: {corrs_valid.min():.4f}\")\n",
    "print(f\"Max r: {corrs_valid.max():.4f}\")\n",
    "print(f\"\\nAll correlations:\")\n",
    "for sub_id, r in sorted(results['subject_deltafc_corr'].items()):\n",
    "    print(f\"  {sub_id}: {r:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7642d38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 13) Visualization: Histogram of correlations\n",
    "# =========================\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "ax.hist(corrs_valid, bins=15, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax.axvline(corrs_valid.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean = {corrs_valid.mean():.3f}')\n",
    "ax.axvline(np.median(corrs_valid), color='green', linestyle='--', linewidth=2, label=f'Median = {np.median(corrs_valid):.3f}')\n",
    "\n",
    "ax.set_xlabel('Per-Subject Î”FC Correlation (r)', fontsize=12)\n",
    "ax.set_ylabel('Number of Subjects', fontsize=12)\n",
    "ax.set_title('Subject-Specific Î”FC: Empirical vs. Simulated (Per-Group Models)', fontsize=13)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad18c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 14) Visualization: Scatter plot (example subject)\n",
    "# =========================\n",
    "if len(results['subject_deltafc_vec_emp']) > 0:\n",
    "    example_sub = list(results['subject_deltafc_vec_emp'].keys())[0]\n",
    "    vec_emp = results['subject_deltafc_vec_emp'][example_sub]\n",
    "    vec_sim = results['subject_deltafc_vec_sim'][example_sub]\n",
    "    r = results['subject_deltafc_corr'][example_sub]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.scatter(vec_emp, vec_sim, s=10, alpha=0.5, color='steelblue')\n",
    "    \n",
    "    # Add regression line\n",
    "    m, b = np.polyfit(vec_emp, vec_sim, 1)\n",
    "    x_line = np.array([vec_emp.min(), vec_emp.max()])\n",
    "    ax.plot(x_line, m*x_line + b, 'r--', linewidth=2, label=f'r = {r:.3f}')\n",
    "    \n",
    "    ax.set_xlabel('Empirical Î”FC', fontsize=11)\n",
    "    ax.set_ylabel('Simulated Î”FC', fontsize=11)\n",
    "    ax.set_title(f'Example: {example_sub} (Per-Group Model)', fontsize=12)\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c57fa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 15) Save validation results\n",
    "# =========================\n",
    "results_summary = {\n",
    "    'n_subjects': len(corrs_valid),\n",
    "    'mean_r_deltafc': float(corrs_valid.mean()),\n",
    "    'std_r_deltafc': float(corrs_valid.std()),\n",
    "    'median_r_deltafc': float(np.median(corrs_valid)),\n",
    "    'min_r_deltafc': float(corrs_valid.min()),\n",
    "    'max_r_deltafc': float(corrs_valid.max()),\n",
    "    'per_subject_correlations': {k: float(v) for k, v in results['subject_deltafc_corr'].items()},\n",
    "}\n",
    "\n",
    "results_path = os.path.join(OUT_DIR, \"deltafc_validation_results.json\")\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(f\"ðŸ’¾ Saved validation results to: {results_path}\")\n",
    "print(f\"\\nâœ… All done!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
