{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56dfd641",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/grabuffo/BrainStim_ANN_fMRI_HCP/blob/main/notebooks/Analyze_TargetRegions_Grouping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d59028a",
   "metadata": {},
   "source": [
    "# TMS-fMRI: Participant Grouping by Target Stimulation Regions\n",
    "\n",
    "This notebook analyzes the target regions for TMS stimulation across all participants and groups them accordingly. We'll identify:\n",
    "- How many unique target regions exist\n",
    "- How many participants are stimulated in each region\n",
    "- Create groupings for subsequent model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed754cde",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f34246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup cell ---\n",
    "\n",
    "# 1Ô∏è‚É£ Mount Google Drive (for data)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# 2Ô∏è‚É£ Clone GitHub repository (for code)\n",
    "import os, sys, subprocess\n",
    "\n",
    "repo_dir = \"/content/BrainStim_ANN_fMRI_HCP\"\n",
    "if not os.path.exists(repo_dir):\n",
    "    !git clone https://github.com/grabuffo/BrainStim_ANN_fMRI_HCP.git\n",
    "else:\n",
    "    print(\"Repo already exists ‚úÖ\")\n",
    "\n",
    "# 3Ô∏è‚É£ Define paths (TMS-fMRI)\n",
    "data_dir = \"/content/drive/MyDrive/Colab Notebooks/Brain_Stim_ANN/data\"\n",
    "preproc_dir = os.path.join(data_dir, \"preprocessed_subjects_tms_fmri\")\n",
    "\n",
    "# 4Ô∏è‚É£ Add repo to import path + imports\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set style for plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"‚úÖ Setup complete | Data directory:\", preproc_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f60f4f",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Load Participant Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b170c240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load all available subjects and their sessions ---\n",
    "\n",
    "# First, get all subject directories\n",
    "subject_dirs = sorted([d for d in os.listdir(preproc_dir) if d.startswith('sub-')])\n",
    "print(f\"Found {len(subject_dirs)} subject directories\")\n",
    "\n",
    "# Check for a metadata file (e.g., participants.tsv or sessions.json)\n",
    "# For now, we'll infer from available files\n",
    "signal_files = sorted(glob.glob(os.path.join(preproc_dir, \"sub-*_signals.npy\")))\n",
    "subject_ids = sorted(list(set([os.path.basename(f).split('_signals')[0] for f in signal_files])))\n",
    "\n",
    "print(f\"\\nSubjects with preprocessed signals: {len(subject_ids)}\")\n",
    "print(\"Subject IDs:\", subject_ids[:5], \"...\" if len(subject_ids) > 5 else \"\")\n",
    "\n",
    "# Check for metadata files that might contain target region info\n",
    "metadata_files = glob.glob(os.path.join(preproc_dir, \"*metadata*\"))\n",
    "print(f\"\\nMetadata files found: {metadata_files}\")\n",
    "\n",
    "# Also check for task-specific files or logs\n",
    "task_stim_files = glob.glob(os.path.join(preproc_dir, \"sub-*_task-stim*\"))\n",
    "print(f\"Task-stim files found: {len(task_stim_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faed5197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# --- Load the TMS-fMRI dataset ---\n",
    "\n",
    "BASE = data_dir\n",
    "DATASET_PKL = os.path.join(BASE, \"TMS_fMRI\", \"dataset_tian50_schaefer400_allruns.pkl\")\n",
    "\n",
    "with open(DATASET_PKL, \"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "print(f\"‚úÖ Loaded dataset from: {DATASET_PKL}\")\n",
    "print(f\"   Dataset keys (sample): {list(dataset.keys())[:3]}\")\n",
    "\n",
    "# Get list of all subjects in the dataset\n",
    "subjects_in_dataset = [k.replace('sub-', '') for k in dataset.keys() if k.startswith('sub-')]\n",
    "print(f\"   Total subjects in dataset: {len(subjects_in_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cb8689",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Extract Target Regions from Metadata\n",
    "\n",
    "**Note:** This cell attempts to load target region information. You may need to adjust based on how the metadata is stored in your dataset (e.g., JSON files, TSV metadata, or embedded in filenames)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41dc576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Extract target regions from dataset ---\n",
    "# Load the dataset (adjust path if needed)\n",
    "\n",
    "# Assuming dataset is loaded or available; if not, you may need to load it from a pickle/npz file\n",
    "# dataset = np.load('path_to_dataset.npy', allow_pickle=True).item()  # if stored as .npy\n",
    "# or\n",
    "# import pickle\n",
    "# with open('path_to_dataset.pkl', 'rb') as f:\n",
    "#     dataset = pickle.load(f)\n",
    "\n",
    "# For now, we'll build participant_targets from the dataset structure:\n",
    "# dataset['sub-<subject_id>']['task-stim'][trial_index] contains 'target' (one-hot encoded)\n",
    "\n",
    "participant_targets = {}  # {subject_id: [list of unique target regions]}\n",
    "\n",
    "for sid_idx, sid in enumerate(subject_ids):\n",
    "    subject_key = f'sub-{sid}' if not sid.startswith('sub-') else sid\n",
    "    \n",
    "    # Try to access task-stim trials for this subject\n",
    "    try:\n",
    "        task_stim_data = dataset[subject_key]['task-stim']\n",
    "        \n",
    "        # Extract target regions from all trials for this subject\n",
    "        targets = []\n",
    "        for trial_idx in range(len(task_stim_data)):\n",
    "            trial_data = task_stim_data[trial_idx]\n",
    "            \n",
    "            # target is one-hot encoded, find which region was stimulated\n",
    "            if 'target' in trial_data:\n",
    "                target_array = trial_data['target']\n",
    "                target_id = np.where(target_array == 1)[0]\n",
    "                \n",
    "                if len(target_id) > 0:\n",
    "                    target_region = int(target_id[0])\n",
    "                    targets.append(target_region)\n",
    "        \n",
    "        # Store unique targets for this subject\n",
    "        if targets:\n",
    "            participant_targets[sid] = sorted(list(set(targets)))\n",
    "            \n",
    "    except KeyError:\n",
    "        print(f\"Warning: No task-stim data found for {subject_key}\")\n",
    "        continue\n",
    "\n",
    "print(f\"Extracted target regions for {len(participant_targets)} subjects\")\n",
    "print(\"\\nExample entries:\")\n",
    "for sid in list(participant_targets.keys())[:3]:\n",
    "    print(f\"  {sid}: {participant_targets[sid]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b90c6a4",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Count Unique Target Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e80bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Count unique target regions and participants per region ---\n",
    "\n",
    "# Flatten all targets to find unique ones\n",
    "all_targets = []\n",
    "for targets in participant_targets.values():\n",
    "    if isinstance(targets, list):\n",
    "        all_targets.extend(targets)\n",
    "    else:\n",
    "        all_targets.append(targets)\n",
    "\n",
    "unique_targets = sorted(list(set(all_targets)))\n",
    "\n",
    "print(f\"Total unique target regions: {len(unique_targets)}\")\n",
    "print(f\"Target regions: {unique_targets}\\n\")\n",
    "\n",
    "# Count participants per target region\n",
    "target_to_participants = defaultdict(list)\n",
    "for sid, targets in participant_targets.items():\n",
    "    if isinstance(targets, list):\n",
    "        for target in targets:\n",
    "            target_to_participants[target].append(sid)\n",
    "    else:\n",
    "        target_to_participants[targets].append(sid)\n",
    "\n",
    "# Sort by number of participants (descending)\n",
    "sorted_targets = sorted(target_to_participants.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "\n",
    "print(\"Participants per target region:\")\n",
    "print(\"-\" * 50)\n",
    "for target, participants in sorted_targets:\n",
    "    print(f\"{target:20s}: {len(participants):3d} participants\")\n",
    "    print(f\"  {participants}\")\n",
    "    \n",
    "print(\"-\" * 50)\n",
    "print(f\"Total: {len(participant_targets)} participants\")\n",
    "\n",
    "# Summary statistics\n",
    "target_counts = [len(p) for p in target_to_participants.values()]\n",
    "print(f\"\\nGroup size statistics:\")\n",
    "print(f\"  Mean group size: {np.mean(target_counts):.2f}\")\n",
    "print(f\"  Median group size: {np.median(target_counts):.1f}\")\n",
    "print(f\"  Min group size: {np.min(target_counts)}\")\n",
    "print(f\"  Max group size: {np.max(target_counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eef4ee6",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Visualize Participant Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e721847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bar plot: participants per target region ---\n",
    "\n",
    "targets_list = [t for t, p in sorted_targets]\n",
    "counts_list = [len(p) for t, p in sorted_targets]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.bar(range(len(targets_list)), counts_list, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, count) in enumerate(zip(bars, counts_list)):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "            str(count), ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel(\"Target Region\", fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel(\"Number of Participants\", fontsize=12, fontweight='bold')\n",
    "ax.set_title(\"Participant Distribution Across Target Regions\", fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(range(len(targets_list)))\n",
    "ax.set_xticklabels(targets_list, rotation=45, ha='right')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1156e923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pie chart: proportion of participants per target region ---\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(targets_list)))\n",
    "\n",
    "wedges, texts, autotexts = ax.pie(counts_list, labels=targets_list, autopct='%1.1f%%',\n",
    "                                    colors=colors, startangle=90, textprops={'fontsize': 10})\n",
    "\n",
    "# Make percentage text bold\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('black')\n",
    "    autotext.set_fontweight('bold')\n",
    "    autotext.set_fontsize(9)\n",
    "\n",
    "ax.set_title(\"Proportion of Participants per Target Region\", fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239d66a5",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Generate Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd81b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create summary DataFrame ---\n",
    "\n",
    "summary_data = []\n",
    "for target, participants in sorted_targets:\n",
    "    summary_data.append({\n",
    "        'Target Region': target,\n",
    "        'N Participants': len(participants),\n",
    "        'Participants': ', '.join(participants),\n",
    "        'Percentage': f\"{100 * len(participants) / len(participant_targets):.1f}%\"\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: TARGET REGIONS & PARTICIPANT GROUPING\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "print(df_summary.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Total participants: {len(participant_targets)}\")\n",
    "print(f\"Total target regions: {len(unique_targets)}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a7beb8",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Export Grouping Information for Model Training\n",
    "\n",
    "Save the grouping information for use in subsequent training notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2133ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save grouping information as JSON ---\n",
    "\n",
    "# Create a clean grouping dictionary\n",
    "grouping = {\n",
    "    'target_regions': unique_targets,\n",
    "    'participant_groups': {target: participants for target, participants in target_to_participants.items()},\n",
    "    'participant_to_target': participant_targets,\n",
    "    'summary': {\n",
    "        'total_participants': len(participant_targets),\n",
    "        'total_target_regions': len(unique_targets),\n",
    "        'mean_group_size': float(np.mean(target_counts)),\n",
    "        'median_group_size': float(np.median(target_counts)),\n",
    "        'min_group_size': int(np.min(target_counts)),\n",
    "        'max_group_size': int(np.max(target_counts))\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "grouping_path = os.path.join(preproc_dir, \"target_region_grouping.json\")\n",
    "with open(grouping_path, 'w') as f:\n",
    "    json.dump(grouping, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Saved grouping information to: {grouping_path}\")\n",
    "\n",
    "# Also save the summary table as CSV\n",
    "csv_path = os.path.join(preproc_dir, \"target_region_summary.csv\")\n",
    "df_summary.to_csv(csv_path, index=False)\n",
    "print(f\"‚úÖ Saved summary table to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a471d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Display final summary ---\n",
    "\n",
    "print(\"\\n\" + \"üéØ \"*20)\n",
    "print(\"GROUPING READY FOR MODEL TRAINING\")\n",
    "print(\"üéØ \"*20 + \"\\n\")\n",
    "\n",
    "print(\"Next steps:\")\n",
    "print(\"1. Use the grouping information to train separate models per target region\")\n",
    "print(\"2. For each target region group:\")\n",
    "print(\"   - Use REST sessions for training data\")\n",
    "print(\"   - Use STIM sessions for validation/evaluation\")\n",
    "print(\"3. Compare models across different target regions\\n\")\n",
    "\n",
    "print(\"Key files generated:\")\n",
    "print(f\"  - {grouping_path}\")\n",
    "print(f\"  - {csv_path}\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
