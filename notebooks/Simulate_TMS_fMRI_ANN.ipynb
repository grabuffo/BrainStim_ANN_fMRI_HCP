{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "196e4d1f",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/grabuffo/BrainStim_ANN_fMRI_HCP/blob/main/notebooks/Simulate_TMS_fMRI_ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5feaf20",
   "metadata": {},
   "source": [
    "# Simulate TMS-fMRI Sessions with Population ANN\n",
    "\n",
    "Generate synthetic TMS-fMRI dataset using the population ANN model trained on task-rest data.\n",
    "\n",
    "## Workflow\n",
    "1. Load empirical dataset and trained population model\n",
    "2. For each subject, simulate REST and STIM runs using autoregressive prediction\n",
    "3. Inject TMS stimulation with spatial spread via Gaussian kernel\n",
    "4. Save synthetic dataset with same structure as empirical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25cfed4",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbeef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "import os, sys, pickle, json, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Clone repo + add to path\n",
    "REPO_DIR = \"/content/BrainStim_ANN_fMRI_HCP\"\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone https://github.com/grabuffo/BrainStim_ANN_fMRI_HCP.git\n",
    "else:\n",
    "    print(\"Repo already exists ✅\")\n",
    "\n",
    "sys.path.append(REPO_DIR)\n",
    "from src.NPI import build_model, device\n",
    "print(f\"PyTorch device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5480c6fe",
   "metadata": {},
   "source": [
    "## Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f586f70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = \"/content/drive/MyDrive/Colab Notebooks/Brain_Stim_ANN/data\"\n",
    "\n",
    "DATASET_EMP_PKL = os.path.join(BASE, \"TMS_fMRI\", \"dataset_tian50_schaefer400_allruns.pkl\")\n",
    "PREPROC_ROOT = os.path.join(BASE, \"preprocessed_subjects_tms_fmri\")\n",
    "MODEL_DIR = os.path.join(PREPROC_ROOT, \"trained_models_MLP_tms_fmri\")\n",
    "\n",
    "# Find population model\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, \"population_MLP_tms_fmri.pt\")\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    raise FileNotFoundError(f\"Population model not found: {MODEL_PATH}\")\n",
    "\n",
    "# Output directory\n",
    "OUT_DIR = os.path.join(PREPROC_ROOT, \"ANN_vs_tms_fmri\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "OUT_PKL = os.path.join(OUT_DIR, \"dataset_simulated_populationANN.pkl\")\n",
    "\n",
    "print(f\"✓ Dataset: {DATASET_EMP_PKL}\")\n",
    "print(f\"✓ Model: {MODEL_PATH}\")\n",
    "print(f\"✓ Output: {OUT_PKL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5a51e1",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ca80b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation parameters\n",
    "S = 3                          # Window length (steps for input)\n",
    "N = 450                        # Number of ROIs (Tian 50 + Schaefer 400)\n",
    "TR_MODEL = 2.0                 # Model TR (seconds)\n",
    "BURN_IN = 10                   # Burn-in steps to stabilize\n",
    "NOISE_SIGMA = 0.28             # Input noise magnitude at each step\n",
    "STIM_AMP = 10.0                # Stimulation amplitude\n",
    "STIM_DURATION_S = TR_MODEL     # TMS pulse duration (seconds)\n",
    "RHO_MM = 10.0                  # Gaussian spread of TMS (mm)\n",
    "MAP_MODE = \"round\"             # Onset mapping mode (round|floor|ceil)\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# Load distance matrix + compute Gaussian kernel for spatial TMS spread\n",
    "DIST_PATH = os.path.join(BASE, \"TMS_fMRI\", \"atlases\", \"distance_matrix_450x450_Tian50_Schaefer400.npy\")\n",
    "D = np.load(DIST_PATH)\n",
    "W = np.exp(-(D ** 2) / (2.0 * (RHO_MM ** 2))).astype(np.float32)\n",
    "W /= (W[np.arange(N), np.arange(N)][:, None] + 1e-8)  # Normalize so target = 1\n",
    "\n",
    "print(f\"Config: S={S}, N={N}, TR={TR_MODEL}s, noise_sigma={NOISE_SIGMA}, stim_amp={STIM_AMP}\")\n",
    "print(f\"Distance matrix: {D.min():.1f}-{D.max():.1f} mm | RHO_MM={RHO_MM}\")\n",
    "print(f\"Gaussian kernel W: shape={W.shape}, range=[{W.min():.4f}, {W.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281dd0c0",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382a69c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onset_column(df):\n",
    "    \"\"\"Find onset column in dataframe.\"\"\"\n",
    "    if df is None or len(df) == 0:\n",
    "        return None\n",
    "    for col in [\"onset\", \"Onset\", \"stim_onset\", \"onset_s\", \"onset_sec\", \"time\", \"t\", \"seconds\"]:\n",
    "        if col in df.columns:\n",
    "            return col\n",
    "    for col in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "def map_onsets_to_steps(onsets_s, tr_model=TR_MODEL, mode=MAP_MODE):\n",
    "    \"\"\"Map stimulus onsets (seconds) to model steps.\"\"\"\n",
    "    onsets_s = np.asarray(onsets_s, dtype=float)\n",
    "    x = onsets_s / float(tr_model)\n",
    "    if mode == \"round\":\n",
    "        steps = np.rint(x).astype(int)\n",
    "    elif mode == \"floor\":\n",
    "        steps = np.floor(x).astype(int)\n",
    "    elif mode == \"ceil\":\n",
    "        steps = np.ceil(x).astype(int)\n",
    "    else:\n",
    "        raise ValueError(\"mode must be round|floor|ceil\")\n",
    "    steps = steps[steps >= 0]\n",
    "    return np.unique(steps)\n",
    "\n",
    "def map_onsets_to_steps_with_duration(onsets_s, duration_s=STIM_DURATION_S, tr_model=TR_MODEL, mode=MAP_MODE):\n",
    "    \"\"\"Map stimulus onsets and duration (seconds) to model steps.\n",
    "    \n",
    "    Creates steps from onset to onset+duration for each stimulus.\n",
    "    \"\"\"\n",
    "    onsets_s = np.asarray(onsets_s, dtype=float)\n",
    "    stim_steps = set()\n",
    "\n",
    "    for onset in onsets_s:\n",
    "        onset_step = onset / float(tr_model)\n",
    "        offset_step = (onset + duration_s) / float(tr_model)\n",
    "\n",
    "        if mode == \"round\":\n",
    "            steps = np.arange(np.rint(onset_step), np.rint(offset_step)).astype(int)\n",
    "        elif mode == \"floor\":\n",
    "            steps = np.arange(np.floor(onset_step), np.floor(offset_step)).astype(int)\n",
    "        elif mode == \"ceil\":\n",
    "            steps = np.arange(np.ceil(onset_step), np.ceil(offset_step)).astype(int)\n",
    "        else:\n",
    "            raise ValueError(\"mode must be round|floor|ceil\")\n",
    "\n",
    "        stim_steps.update(steps[steps >= 0])\n",
    "\n",
    "    return np.unique(sorted(stim_steps))\n",
    "\n",
    "def safe_target_idx(target_vec):\n",
    "    \"\"\"Extract target region index from one-hot vector.\"\"\"\n",
    "    if target_vec is None:\n",
    "        return None\n",
    "    v = np.asarray(target_vec).astype(int).ravel()\n",
    "    if v.size == 0 or v.sum() != 1:\n",
    "        return None\n",
    "    return int(np.argmax(v))\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_next(model, window_SxN):\n",
    "    \"\"\"Predict next state with input noise.\"\"\"\n",
    "    x_np = window_SxN.reshape(-1).astype(np.float32)\n",
    "    noise = NOISE_SIGMA * rng.normal(0.0, 1.0, size=x_np.shape).astype(np.float32)\n",
    "    x_np = x_np + noise\n",
    "    x = torch.tensor(x_np[None, :], dtype=torch.float32, device=device)\n",
    "    y = model(x)\n",
    "    return y.detach().cpu().numpy().squeeze(0)\n",
    "\n",
    "def simulate_run(model, init_window_SxN, n_steps, stim_steps=None, target_idx=None, W=None):\n",
    "    \"\"\"Simulate brain activity time series with optional TMS stimulation.\n",
    "    \n",
    "    Args:\n",
    "        model: trained ANN model\n",
    "        init_window_SxN: (S, N) initial state\n",
    "        n_steps: number of simulation steps\n",
    "        stim_steps: set of steps to apply stimulation\n",
    "        target_idx: target region for stimulation\n",
    "        W: (N, N) spatial Gaussian kernel for TMS spread\n",
    "    \n",
    "    Returns:\n",
    "        sim_ts: (n_steps, N) simulated time series\n",
    "        meta_sim: dict of simulation metadata\n",
    "    \"\"\"\n",
    "    init_window_SxN = np.asarray(init_window_SxN, dtype=np.float32)\n",
    "    assert init_window_SxN.shape == (S, N)\n",
    "\n",
    "    stim_steps = set(int(s) for s in (stim_steps or []))\n",
    "    do_stim = (target_idx is not None) and (len(stim_steps) > 0)\n",
    "    w = init_window_SxN.copy()\n",
    "\n",
    "    # Burn-in: stabilize initial state\n",
    "    for _ in range(BURN_IN):\n",
    "        y = predict_next(model, w)\n",
    "        w = np.vstack([w[1:], y[None, :]])\n",
    "\n",
    "    # Simulation loop with optional stimulation\n",
    "    out = np.zeros((n_steps, N), dtype=np.float32)\n",
    "    for t in range(n_steps):\n",
    "        w_in = w.copy()\n",
    "        if do_stim and (t in stim_steps):\n",
    "            if W is None:\n",
    "                # Direct stimulation: single target\n",
    "                w_in[-1, target_idx] += STIM_AMP\n",
    "            else:\n",
    "                # Spatial spread: apply Gaussian kernel\n",
    "                w_in[-1, :] += STIM_AMP * W[target_idx, :]\n",
    "        y = predict_next(model, w_in)\n",
    "        out[t] = y\n",
    "        w = np.vstack([w[1:], y[None, :]])\n",
    "\n",
    "    meta_sim = {\n",
    "        \"tr_model_s\": float(TR_MODEL),\n",
    "        \"burn_in_steps\": int(BURN_IN),\n",
    "        \"noise_input_sigma\": float(NOISE_SIGMA),\n",
    "        \"stim_amp\": float(STIM_AMP),\n",
    "        \"stim_steps_modelTR\": sorted(list(stim_steps)) if do_stim else [],\n",
    "        \"stim_mapping_mode\": MAP_MODE,\n",
    "    }\n",
    "    return out, meta_sim\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd951d72",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb3d654",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading model from {MODEL_PATH}...\")\n",
    "model = build_model(\"MLP\", ROI_num=N, using_steps=S).to(device)\n",
    "\n",
    "try:\n",
    "    state = torch.load(MODEL_PATH, map_location=device, weights_only=True)\n",
    "    if isinstance(state, dict) and \"state_dict\" in state:\n",
    "        model.load_state_dict(state[\"state_dict\"])\n",
    "    elif isinstance(state, dict):\n",
    "        model.load_state_dict(state)\n",
    "    else:\n",
    "        raise RuntimeError(\"Unexpected format\")\n",
    "except Exception as e:\n",
    "    print(f\"weights_only=True failed, using weights_only=False: {e}\")\n",
    "    state = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
    "    if isinstance(state, dict) and \"state_dict\" in state:\n",
    "        model.load_state_dict(state[\"state_dict\"])\n",
    "    elif isinstance(state, dict):\n",
    "        model.load_state_dict(state)\n",
    "    else:\n",
    "        model = state.to(device)\n",
    "\n",
    "model.eval()\n",
    "print(\"✓ Model loaded and ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efec65b2",
   "metadata": {},
   "source": [
    "## Load Empirical Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3d106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading empirical dataset from {DATASET_EMP_PKL}...\")\n",
    "with open(DATASET_EMP_PKL, \"rb\") as f:\n",
    "    dataset_emp = pickle.load(f)\n",
    "\n",
    "print(f\"✓ Loaded {len(dataset_emp)} subjects\")\n",
    "print(f\"  First subject: {list(dataset_emp.keys())[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ba3cd",
   "metadata": {},
   "source": [
    "## Generate Synthetic Dataset\n",
    "\n",
    "For each subject, simulate REST and STIM runs using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b59338",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating synthetic dataset...\\n\")\n",
    "\n",
    "dataset_sim = {}\n",
    "n_sim_rest = 0\n",
    "n_sim_stim = 0\n",
    "\n",
    "for sub_id, sub_data in dataset_emp.items():\n",
    "    dataset_sim[sub_id] = {\"task-rest\": {}, \"task-stim\": {}}\n",
    "\n",
    "    # ---- SIMULATE REST ----\n",
    "    if \"task-rest\" in sub_data:\n",
    "        for run_idx, run in sub_data[\"task-rest\"].items():\n",
    "            ts_emp = run.get(\"time series\", None)\n",
    "            md_emp = run.get(\"metadata\", {}) or {}\n",
    "\n",
    "            if ts_emp is None or not isinstance(ts_emp, np.ndarray) or ts_emp.shape[1] != N:\n",
    "                continue\n",
    "\n",
    "            tr_emp = float(md_emp.get(\"tr_s\", 2.0))\n",
    "            dur_s = ts_emp.shape[0] * tr_emp\n",
    "            n_steps = int(math.ceil(dur_s / TR_MODEL))\n",
    "\n",
    "            init_window = ts_emp[:S].copy()\n",
    "            sim_ts, meta_sim = simulate_run(model, init_window, n_steps)\n",
    "\n",
    "            md_out = dict(md_emp)\n",
    "            md_out.update({\n",
    "                \"simulated\": True,\n",
    "                \"duration_emp_s\": float(dur_s),\n",
    "                \"n_steps_model\": int(n_steps),\n",
    "                **meta_sim\n",
    "            })\n",
    "\n",
    "            dataset_sim[sub_id][\"task-rest\"][int(run_idx)] = {\n",
    "                \"time series\": sim_ts,\n",
    "                \"metadata\": md_out\n",
    "            }\n",
    "            n_sim_rest += 1\n",
    "\n",
    "    # ---- SIMULATE STIM ----\n",
    "    if \"task-stim\" in sub_data:\n",
    "        for run_idx, run in sub_data[\"task-stim\"].items():\n",
    "            ts_emp = run.get(\"time series\", None)\n",
    "            md_emp = run.get(\"metadata\", {}) or {}\n",
    "            target_vec = run.get(\"target\", None)\n",
    "            events_df = run.get(\"stim time\", None)\n",
    "\n",
    "            if ts_emp is None or not isinstance(ts_emp, np.ndarray) or ts_emp.shape[1] != N:\n",
    "                continue\n",
    "\n",
    "            target_idx = safe_target_idx(target_vec)\n",
    "            if target_idx is None:\n",
    "                continue\n",
    "\n",
    "            onset_col = get_onset_column(events_df) if isinstance(events_df, pd.DataFrame) else None\n",
    "            if onset_col is None:\n",
    "                continue\n",
    "\n",
    "            onsets_s = events_df[onset_col].astype(float).values\n",
    "            stim_steps = list(map_onsets_to_steps_with_duration(onsets_s, duration_s=STIM_DURATION_S))\n",
    "\n",
    "            tr_emp = float(md_emp.get(\"tr_s\", 2.4))\n",
    "            dur_s = ts_emp.shape[0] * tr_emp\n",
    "            n_steps = int(math.ceil(dur_s / TR_MODEL))\n",
    "\n",
    "            init_window = ts_emp[:S].copy()\n",
    "            sim_ts, meta_sim = simulate_run(model, init_window, n_steps,\n",
    "                                            stim_steps=stim_steps, target_idx=target_idx, W=W)\n",
    "\n",
    "            md_out = dict(md_emp)\n",
    "            md_out.update({\n",
    "                \"simulated\": True,\n",
    "                \"duration_emp_s\": float(dur_s),\n",
    "                \"n_steps_model\": int(n_steps),\n",
    "                \"target_idx\": int(target_idx),\n",
    "                **meta_sim\n",
    "            })\n",
    "\n",
    "            dataset_sim[sub_id][\"task-stim\"][int(run_idx)] = {\n",
    "                \"time series\": sim_ts,\n",
    "                \"metadata\": md_out,\n",
    "                \"target\": target_vec,\n",
    "                \"stim time\": events_df,\n",
    "            }\n",
    "            n_sim_stim += 1\n",
    "\n",
    "print(f\"✓ Generated {n_sim_rest} rest runs, {n_sim_stim} stim runs\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d70830",
   "metadata": {},
   "source": [
    "## Save Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ddd036",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Saving synthetic dataset to {OUT_PKL}...\")\n",
    "with open(OUT_PKL, \"wb\") as f:\n",
    "    pickle.dump(dataset_sim, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(\"✓ Saved\")\n",
    "print(f\"\\nOutput file size: {os.path.getsize(OUT_PKL) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6a3ce7",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The synthetic dataset has been successfully created with the same hierarchical structure as the empirical data:\n",
    "\n",
    "```\n",
    "dataset_sim[sub_id]['task-rest'][run_idx] = {\n",
    "    'time series': (n_steps, 450),\n",
    "    'metadata': {...}\n",
    "}\n",
    "\n",
    "dataset_sim[sub_id]['task-stim'][run_idx] = {\n",
    "    'time series': (n_steps, 450),\n",
    "    'metadata': {...},\n",
    "    'target': one-hot(450,),\n",
    "    'stim time': DataFrame\n",
    "}\n",
    "```\n",
    "\n",
    "Ready for validation analysis!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
