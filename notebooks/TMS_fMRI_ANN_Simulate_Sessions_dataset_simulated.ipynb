{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56464e66-5cbe-4602-8c19-92e7f3d0c262",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/grabuffo/BrainStim_ANN_fMRI_HCP/blob/main/notebooks/TMS_fMRI_ANN_Simulate_Sessions_dataset_simulated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa45402-2405-4b7e-a92d-18376a3d7e57",
   "metadata": {},
   "source": [
    "# Simulate TMS-fMRI sessions with population ANN (dataset_simulated)\n",
    "\n",
    "Creates a `dataset_simulated` dictionary mirroring the empirical `dataset` structure and saves it to Google Drive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e2bcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# 0) Mount Google Drive\n",
    "# =========================\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c558fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# 1) Clone repo + imports\n",
    "# =========================\n",
    "import os, sys, pickle, json, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "REPO_DIR = \"/content/BrainStim_ANN_fMRI_HCP\"\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone https://github.com/grabuffo/BrainStim_ANN_fMRI_HCP.git\n",
    "else:\n",
    "    print(\"Repo already exists âœ…\")\n",
    "\n",
    "sys.path.append(REPO_DIR)\n",
    "\n",
    "from src.NPI import build_model, device\n",
    "print(\"Torch device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a2c76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# 2) Paths (EDIT IF NEEDED)\n",
    "# =========================\n",
    "BASE = \"/content/drive/MyDrive/Colab Notebooks/Brain_Stim_ANN/data\"\n",
    "\n",
    "DATASET_EMP_PKL = os.path.join(BASE, \"TMS_fMRI\", \"dataset_tian50_schaefer400_allruns.pkl\")\n",
    "\n",
    "PREPROC_ROOT = os.path.join(BASE, \"preprocessed_subjects_tms_fmri\")\n",
    "MODEL_DIR = os.path.join(PREPROC_ROOT, \"trained_models_MLP_tms_fmri\")\n",
    "\n",
    "MODEL_PATH = None\n",
    "for cand in [\n",
    "    os.path.join(MODEL_DIR, \"population_MLP_tms_fmri.pt\"),\n",
    "    os.path.join(MODEL_DIR, \"population_MLP_tms_fmri.pth\"),\n",
    "]:\n",
    "    if os.path.exists(cand):\n",
    "        MODEL_PATH = cand\n",
    "        break\n",
    "\n",
    "OUT_DIR = os.path.join(PREPROC_ROOT, \"ANN_vs_tms_fmri\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "OUT_PKL = os.path.join(OUT_DIR, \"dataset_simulated_populationANN.pkl\")\n",
    "\n",
    "print(\"Empirical dataset:\", DATASET_EMP_PKL, \"| exists:\", os.path.exists(DATASET_EMP_PKL))\n",
    "print(\"Model:\", MODEL_PATH)\n",
    "print(\"Will save to:\", OUT_PKL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3e639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# 3) Load empirical dataset\n",
    "# =========================\n",
    "with open(DATASET_EMP_PKL, \"rb\") as f:\n",
    "    dataset_emp = pickle.load(f)\n",
    "\n",
    "print(\"Loaded subjects:\", len(dataset_emp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1770d034",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# 4) Config\n",
    "# =========================\n",
    "S = 3\n",
    "N = 450\n",
    "\n",
    "TR_MODEL = 2.0          # model step in seconds (trained on rest TR=2s)\n",
    "BURN_IN = 100           # steps (not saved)\n",
    "NOISE_SIGMA = 0.01      # z-scored units\n",
    "STIM_AMP = 0.1          # z-scored units\n",
    "\n",
    "MAP_MODE = \"round\"      # \"round\" | \"floor\" | \"ceil\" for mapping onset_s -> model steps\n",
    "\n",
    "rng = np.random.default_rng(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b19712",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# 5) Helpers\n",
    "# =========================\n",
    "def get_onset_column(df: pd.DataFrame):\n",
    "    if df is None or len(df) == 0:\n",
    "        return None\n",
    "    for c in [\"onset\", \"Onset\", \"stim_onset\", \"onset_s\", \"onset_sec\", \"time\", \"t\", \"seconds\"]:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    for c in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[c]):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def map_onsets_to_steps(onsets_s, tr_model=TR_MODEL, mode=MAP_MODE):\n",
    "    onsets_s = np.asarray(onsets_s, dtype=float)\n",
    "    x = onsets_s / float(tr_model)\n",
    "    if mode == \"round\":\n",
    "        steps = np.rint(x).astype(int)\n",
    "    elif mode == \"floor\":\n",
    "        steps = np.floor(x).astype(int)\n",
    "    elif mode == \"ceil\":\n",
    "        steps = np.ceil(x).astype(int)\n",
    "    else:\n",
    "        raise ValueError(\"mode must be round|floor|ceil\")\n",
    "    steps = steps[steps >= 0]\n",
    "    return np.unique(steps)\n",
    "\n",
    "def safe_target_idx(target_vec):\n",
    "    if target_vec is None:\n",
    "        return None\n",
    "    v = np.asarray(target_vec).astype(int).ravel()\n",
    "    if v.size == 0 or v.sum() != 1:\n",
    "        return None\n",
    "    return int(np.argmax(v))\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_next(model, window_SxN: np.ndarray):\n",
    "    x = torch.tensor(window_SxN.reshape(1, -1), dtype=torch.float32, device=device)\n",
    "    y = model(x)\n",
    "    return y.detach().cpu().numpy().squeeze(0)\n",
    "\n",
    "def simulate_run(model, init_window_SxN, n_steps, stim_steps=None, target_idx=None,\n",
    "                 stim_amp=STIM_AMP, noise_sigma=NOISE_SIGMA, burn_in=BURN_IN):\n",
    "    init_window_SxN = np.asarray(init_window_SxN, dtype=np.float32)\n",
    "    assert init_window_SxN.shape == (S, N)\n",
    "\n",
    "    stim_steps = set(int(s) for s in (stim_steps or []))\n",
    "    do_stim = (target_idx is not None) and (len(stim_steps) > 0)\n",
    "\n",
    "    w = init_window_SxN.copy()\n",
    "\n",
    "    # burn-in\n",
    "    for _ in range(burn_in):\n",
    "        y = predict_next(model, w)\n",
    "        if noise_sigma > 0:\n",
    "            y = y + rng.normal(0, noise_sigma, size=y.shape).astype(np.float32)\n",
    "        w = np.vstack([w[1:], y[None, :]])\n",
    "\n",
    "    # session\n",
    "    out = np.zeros((n_steps, N), dtype=np.float32)\n",
    "    for t in range(n_steps):\n",
    "        w_in = w.copy()\n",
    "        if do_stim and (t in stim_steps):\n",
    "            w_in[-1, target_idx] += stim_amp\n",
    "        y = predict_next(model, w_in)\n",
    "        if noise_sigma > 0:\n",
    "            y = y + rng.normal(0, noise_sigma, size=y.shape).astype(np.float32)\n",
    "        out[t] = y\n",
    "        w = np.vstack([w[1:], y[None, :]])\n",
    "\n",
    "    meta_sim = {\n",
    "        \"tr_model_s\": float(TR_MODEL),\n",
    "        \"burn_in_steps\": int(burn_in),\n",
    "        \"noise_sigma\": float(noise_sigma),\n",
    "        \"stim_amp\": float(stim_amp),\n",
    "        \"stim_steps_modelTR\": sorted(list(stim_steps)) if do_stim else [],\n",
    "        \"stim_mapping_mode\": MAP_MODE,\n",
    "    }\n",
    "    return out, meta_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8983f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# 6) Load model (robust to PyTorch 2.6+)\n",
    "# =========================\n",
    "if MODEL_PATH is None:\n",
    "    raise FileNotFoundError(\"Could not find population model checkpoint in MODEL_DIR.\")\n",
    "\n",
    "METHOD = \"MLP\"\n",
    "model = build_model(METHOD, ROI_num=N, using_steps=S).to(device)\n",
    "\n",
    "try:\n",
    "    state = torch.load(MODEL_PATH, map_location=device, weights_only=True)\n",
    "    if isinstance(state, dict) and \"state_dict\" in state:\n",
    "        model.load_state_dict(state[\"state_dict\"])\n",
    "    elif isinstance(state, dict):\n",
    "        model.load_state_dict(state)\n",
    "    else:\n",
    "        raise RuntimeError(\"weights_only=True returned non-dict; falling back.\")\n",
    "    print(\"Loaded weights with weights_only=True\")\n",
    "except Exception as e:\n",
    "    print(\"weights_only=True failed; using weights_only=False\")\n",
    "    print(\"Reason:\", repr(e))\n",
    "    state = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
    "    if isinstance(state, dict) and \"state_dict\" in state:\n",
    "        model.load_state_dict(state[\"state_dict\"])\n",
    "    elif isinstance(state, dict):\n",
    "        model.load_state_dict(state)\n",
    "    else:\n",
    "        model = state.to(device)\n",
    "\n",
    "model.eval()\n",
    "print(\"Model ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a46816",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# 7) Build dataset_simulated\n",
    "# =========================\n",
    "dataset_sim = {}\n",
    "n_sim_runs = 0\n",
    "\n",
    "for sub_id, sub_data in dataset_emp.items():\n",
    "    dataset_sim[sub_id] = {\"task-rest\": {}, \"task-stim\": {}}\n",
    "\n",
    "    # ---- REST ----\n",
    "    if \"task-rest\" in sub_data:\n",
    "        for run_idx, run in sub_data[\"task-rest\"].items():\n",
    "            ts_emp = run.get(\"time series\", None)\n",
    "            md_emp = run.get(\"metadata\", {}) or {}\n",
    "            if ts_emp is None or not isinstance(ts_emp, np.ndarray) or ts_emp.shape[1] != N:\n",
    "                continue\n",
    "\n",
    "            tr_emp = float(md_emp.get(\"tr_s\", 2.0))\n",
    "            dur_s = ts_emp.shape[0] * tr_emp\n",
    "            n_steps = int(math.ceil(dur_s / TR_MODEL))\n",
    "\n",
    "            init_window = ts_emp[:S].copy()\n",
    "            sim_ts, meta_sim = simulate_run(model, init_window, n_steps)\n",
    "\n",
    "            md_out = dict(md_emp)\n",
    "            md_out.update({\n",
    "                \"simulated\": True,\n",
    "                \"source_empirical_task\": \"task-rest\",\n",
    "                \"source_run_idx\": int(run_idx),\n",
    "                \"duration_emp_s\": float(dur_s),\n",
    "                \"n_steps_model\": int(n_steps),\n",
    "                **meta_sim\n",
    "            })\n",
    "\n",
    "            dataset_sim[sub_id][\"task-rest\"][int(run_idx)] = {\n",
    "                \"time series\": sim_ts,\n",
    "                \"metadata\": md_out\n",
    "            }\n",
    "            n_sim_runs += 1\n",
    "\n",
    "    # ---- STIM ----\n",
    "    if \"task-stim\" in sub_data:\n",
    "        for run_idx, run in sub_data[\"task-stim\"].items():\n",
    "            ts_emp = run.get(\"time series\", None)\n",
    "            md_emp = run.get(\"metadata\", {}) or {}\n",
    "            target_vec = run.get(\"target\", None)\n",
    "            events_df = run.get(\"stim time\", None)\n",
    "\n",
    "            if ts_emp is None or not isinstance(ts_emp, np.ndarray) or ts_emp.shape[1] != N:\n",
    "                continue\n",
    "\n",
    "            target_idx = safe_target_idx(target_vec)\n",
    "            if target_idx is None:\n",
    "                continue\n",
    "\n",
    "            onset_col = get_onset_column(events_df) if isinstance(events_df, pd.DataFrame) else None\n",
    "            if onset_col is None:\n",
    "                continue\n",
    "\n",
    "            onsets_s = events_df[onset_col].astype(float).values\n",
    "            stim_steps = map_onsets_to_steps(onsets_s, tr_model=TR_MODEL, mode=MAP_MODE)\n",
    "\n",
    "            tr_emp = float(md_emp.get(\"tr_s\", 2.4))\n",
    "            dur_s = ts_emp.shape[0] * tr_emp\n",
    "            n_steps = int(math.ceil(dur_s / TR_MODEL))\n",
    "\n",
    "            init_window = ts_emp[:S].copy()\n",
    "            sim_ts, meta_sim = simulate_run(model, init_window, n_steps,\n",
    "                                            stim_steps=stim_steps, target_idx=target_idx)\n",
    "\n",
    "            md_out = dict(md_emp)\n",
    "            md_out.update({\n",
    "                \"simulated\": True,\n",
    "                \"source_empirical_task\": \"task-stim\",\n",
    "                \"source_run_idx\": int(run_idx),\n",
    "                \"duration_emp_s\": float(dur_s),\n",
    "                \"n_steps_model\": int(n_steps),\n",
    "                \"target_idx\": int(target_idx),\n",
    "                **meta_sim\n",
    "            })\n",
    "\n",
    "            dataset_sim[sub_id][\"task-stim\"][int(run_idx)] = {\n",
    "                \"time series\": sim_ts,\n",
    "                \"metadata\": md_out,\n",
    "                \"target\": target_vec,\n",
    "                \"stim time\": events_df,\n",
    "            }\n",
    "            n_sim_runs += 1\n",
    "\n",
    "print(\"Simulated runs:\", n_sim_runs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957cb22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# 8) Save to Drive\n",
    "# =========================\n",
    "with open(OUT_PKL, \"wb\") as f:\n",
    "    pickle.dump(dataset_sim, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(\"Saved dataset_simulated to:\", OUT_PKL)\n",
    "\n",
    "# Quick peek\n",
    "some_sub = next(iter(dataset_sim.keys()), None)\n",
    "if some_sub:\n",
    "    print(\"Example subject:\", some_sub)\n",
    "    print(\"task-rest runs:\", list(dataset_sim[some_sub][\"task-rest\"].keys())[:5])\n",
    "    print(\"task-stim runs:\", list(dataset_sim[some_sub][\"task-stim\"].keys())[:5])\n",
    "    if dataset_sim[some_sub][\"task-stim\"]:\n",
    "        r0 = next(iter(dataset_sim[some_sub][\"task-stim\"].keys()))\n",
    "        md0 = dataset_sim[some_sub][\"task-stim\"][r0][\"metadata\"]\n",
    "        print(\"Example stim target_idx:\", md0.get(\"target_idx\"))\n",
    "        print(\"Example stim mapped steps (first 10):\", md0.get(\"stim_steps_modelTR\", [])[:10])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (neuro310)",
   "language": "python",
   "name": "neuro310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
