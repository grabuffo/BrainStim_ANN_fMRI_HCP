{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "710a48b2",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/grabuffo/BrainStim_ANN_fMRI_HCP/blob/main/notebooks/TMS_fMRI_ANN_Simulate_Sessions_dataset_simulated_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f92ed82",
   "metadata": {},
   "source": [
    "# Simulate TMS-fMRI Sessions with Population ANN\n",
    "\n",
    "Generate synthetic TMS-fMRI dataset using the population ANN model trained on task-rest data.\n",
    "\n",
    "**Workflow:**\n",
    "1. Load empirical dataset and population model\n",
    "2. Generate synthetic rest + stim sessions for all subjects\n",
    "3. Validate with subject-specific ΔFC analysis (empirical vs simulated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6b2f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# SETUP\n",
    "# =========================\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "import os, sys, pickle, json, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Clone repo + add to path\n",
    "REPO_DIR = \"/content/BrainStim_ANN_fMRI_HCP\"\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone https://github.com/grabuffo/BrainStim_ANN_fMRI_HCP.git\n",
    "else:\n",
    "    print(\"Repo already exists ✅\")\n",
    "\n",
    "sys.path.append(REPO_DIR)\n",
    "from src.NPI import build_model, device\n",
    "print(f\"PyTorch device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cff4b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "BASE = \"/content/drive/MyDrive/Colab Notebooks/Brain_Stim_ANN/data\"\n",
    "\n",
    "DATASET_EMP_PKL = os.path.join(BASE, \"TMS_fMRI\", \"dataset_tian50_schaefer400_allruns.pkl\")\n",
    "PREPROC_ROOT = os.path.join(BASE, \"preprocessed_subjects_tms_fmri\")\n",
    "MODEL_DIR = os.path.join(PREPROC_ROOT, \"trained_models_MLP_tms_fmri\")\n",
    "\n",
    "# Find population model\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, \"population_MLP_tms_fmri.pt\")\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    raise FileNotFoundError(f\"Population model not found: {MODEL_PATH}\")\n",
    "\n",
    "# Output directory\n",
    "OUT_DIR = os.path.join(PREPROC_ROOT, \"ANN_vs_tms_fmri\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "OUT_PKL = os.path.join(OUT_DIR, \"dataset_simulated_populationANN.pkl\")\n",
    "RESULTS_JSON = os.path.join(OUT_DIR, \"deltafc_validation_results.json\")\n",
    "\n",
    "print(f\"✓ Dataset: {DATASET_EMP_PKL}\")\n",
    "print(f\"✓ Model: {MODEL_PATH}\")\n",
    "print(f\"✓ Output: {OUT_PKL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d517d5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "S = 3                          # Window length (steps)\n",
    "N = 450                        # Number of ROIs\n",
    "TR_MODEL = 2.0                 # Model TR (seconds)\n",
    "BURN_IN = 10                   # Burn-in steps\n",
    "NOISE_SIGMA = 0.3              # Input noise magnitude\n",
    "STIM_AMP = 1.0                 # Stimulation amplitude\n",
    "RHO_MM = 10.0                  # Gaussian spread (mm)\n",
    "MAP_MODE = \"round\"             # Onset mapping mode\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# Load distance matrix + compute Gaussian kernel\n",
    "DIST_PATH = os.path.join(BASE, \"TMS_fMRI\", \"atlases\", \"distance_matrix_450x450_Tian50_Schaefer400.npy\")\n",
    "D = np.load(DIST_PATH)\n",
    "W = np.exp(-(D ** 2) / (2.0 * (RHO_MM ** 2))).astype(np.float32)\n",
    "W /= (W[np.arange(N), np.arange(N)][:, None] + 1e-8)  # Normalize so target = 1\n",
    "\n",
    "print(f\"Config: S={S}, N={N}, TR={TR_MODEL}s, noise={NOISE_SIGMA}, stim_amp={STIM_AMP}\")\n",
    "print(f\"Distance matrix: {D.min():.1f}-{D.max():.1f} mm | RHO_MM={RHO_MM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a083cd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# HELPER FUNCTIONS\n",
    "# =========================\n",
    "\n",
    "def get_onset_column(df):\n",
    "    \"\"\"Find onset column in dataframe.\"\"\"\n",
    "    if df is None or len(df) == 0:\n",
    "        return None\n",
    "    for col in [\"onset\", \"Onset\", \"stim_onset\", \"onset_s\", \"onset_sec\", \"time\", \"t\", \"seconds\"]:\n",
    "        if col in df.columns:\n",
    "            return col\n",
    "    for col in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "def map_onsets_to_steps(onsets_s, tr_model=TR_MODEL, mode=MAP_MODE):\n",
    "    \"\"\"Map stimulus onsets (seconds) to model steps.\"\"\"\n",
    "    onsets_s = np.asarray(onsets_s, dtype=float)\n",
    "    x = onsets_s / float(tr_model)\n",
    "    if mode == \"round\":\n",
    "        steps = np.rint(x).astype(int)\n",
    "    elif mode == \"floor\":\n",
    "        steps = np.floor(x).astype(int)\n",
    "    elif mode == \"ceil\":\n",
    "        steps = np.ceil(x).astype(int)\n",
    "    else:\n",
    "        raise ValueError(\"mode must be round|floor|ceil\")\n",
    "    steps = steps[steps >= 0]\n",
    "    return np.unique(steps)\n",
    "\n",
    "def safe_target_idx(target_vec):\n",
    "    \"\"\"Extract target region index from one-hot vector.\"\"\"\n",
    "    if target_vec is None:\n",
    "        return None\n",
    "    v = np.asarray(target_vec).astype(int).ravel()\n",
    "    if v.size == 0 or v.sum() != 1:\n",
    "        return None\n",
    "    return int(np.argmax(v))\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_next(model, window_SxN):\n",
    "    \"\"\"Predict next state with input noise.\"\"\"\n",
    "    x_np = window_SxN.reshape(-1).astype(np.float32)\n",
    "    noise = NOISE_SIGMA * rng.normal(0.0, 1.0, size=x_np.shape).astype(np.float32)\n",
    "    x_np = x_np + noise\n",
    "    x = torch.tensor(x_np[None, :], dtype=torch.float32, device=device)\n",
    "    y = model(x)\n",
    "    return y.detach().cpu().numpy().squeeze(0)\n",
    "\n",
    "def simulate_run(model, init_window_SxN, n_steps, stim_steps=None, target_idx=None, W=None):\n",
    "    \"\"\"Simulate brain activity time series with optional stimulation.\"\"\"\n",
    "    init_window_SxN = np.asarray(init_window_SxN, dtype=np.float32)\n",
    "    assert init_window_SxN.shape == (S, N)\n",
    "    \n",
    "    stim_steps = set(int(s) for s in (stim_steps or []))\n",
    "    do_stim = (target_idx is not None) and (len(stim_steps) > 0)\n",
    "    w = init_window_SxN.copy()\n",
    "    \n",
    "    # Burn-in\n",
    "    for _ in range(BURN_IN):\n",
    "        y = predict_next(model, w)\n",
    "        w = np.vstack([w[1:], y[None, :]])\n",
    "    \n",
    "    # Simulate\n",
    "    out = np.zeros((n_steps, N), dtype=np.float32)\n",
    "    for t in range(n_steps):\n",
    "        w_in = w.copy()\n",
    "        if do_stim and (t in stim_steps):\n",
    "            if W is None:\n",
    "                w_in[-1, target_idx] += STIM_AMP\n",
    "            else:\n",
    "                w_in[-1, :] += STIM_AMP * W[target_idx, :]\n",
    "        y = predict_next(model, w_in)\n",
    "        out[t] = y\n",
    "        w = np.vstack([w[1:], y[None, :]])\n",
    "    \n",
    "    meta_sim = {\n",
    "        \"tr_model_s\": float(TR_MODEL),\n",
    "        \"burn_in_steps\": int(BURN_IN),\n",
    "        \"noise_input_sigma\": float(NOISE_SIGMA),\n",
    "        \"stim_amp\": float(STIM_AMP),\n",
    "        \"stim_steps_modelTR\": sorted(list(stim_steps)) if do_stim else [],\n",
    "        \"stim_mapping_mode\": MAP_MODE,\n",
    "    }\n",
    "    return out, meta_sim\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01be774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# LOAD MODEL\n",
    "# =========================\n",
    "print(f\"Loading model from {MODEL_PATH}...\")\n",
    "model = build_model(\"MLP\", ROI_num=N, using_steps=S).to(device)\n",
    "\n",
    "try:\n",
    "    state = torch.load(MODEL_PATH, map_location=device, weights_only=True)\n",
    "    if isinstance(state, dict) and \"state_dict\" in state:\n",
    "        model.load_state_dict(state[\"state_dict\"])\n",
    "    elif isinstance(state, dict):\n",
    "        model.load_state_dict(state)\n",
    "    else:\n",
    "        raise RuntimeError(\"Unexpected format\")\n",
    "except Exception as e:\n",
    "    print(f\"weights_only=True failed, using weights_only=False: {e}\")\n",
    "    state = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
    "    if isinstance(state, dict) and \"state_dict\" in state:\n",
    "        model.load_state_dict(state[\"state_dict\"])\n",
    "    elif isinstance(state, dict):\n",
    "        model.load_state_dict(state)\n",
    "    else:\n",
    "        model = state.to(device)\n",
    "\n",
    "model.eval()\n",
    "print(\"✓ Model loaded and ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ce82eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# LOAD EMPIRICAL DATASET\n",
    "# =========================\n",
    "print(f\"Loading empirical dataset from {DATASET_EMP_PKL}...\")\n",
    "with open(DATASET_EMP_PKL, \"rb\") as f:\n",
    "    dataset_emp = pickle.load(f)\n",
    "\n",
    "print(f\"✓ Loaded {len(dataset_emp)} subjects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d50afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# GENERATE SYNTHETIC DATASET\n",
    "# =========================\n",
    "print(\"Generating synthetic dataset...\\n\")\n",
    "\n",
    "dataset_sim = {}\n",
    "n_sim_rest = 0\n",
    "n_sim_stim = 0\n",
    "\n",
    "for sub_id, sub_data in dataset_emp.items():\n",
    "    dataset_sim[sub_id] = {\"task-rest\": {}, \"task-stim\": {}}\n",
    "    \n",
    "    # ---- SIMULATE REST ----\n",
    "    if \"task-rest\" in sub_data:\n",
    "        for run_idx, run in sub_data[\"task-rest\"].items():\n",
    "            ts_emp = run.get(\"time series\", None)\n",
    "            md_emp = run.get(\"metadata\", {}) or {}\n",
    "            \n",
    "            if ts_emp is None or not isinstance(ts_emp, np.ndarray) or ts_emp.shape[1] != N:\n",
    "                continue\n",
    "            \n",
    "            tr_emp = float(md_emp.get(\"tr_s\", 2.0))\n",
    "            dur_s = ts_emp.shape[0] * tr_emp\n",
    "            n_steps = int(math.ceil(dur_s / TR_MODEL))\n",
    "            \n",
    "            init_window = ts_emp[:S].copy()\n",
    "            sim_ts, meta_sim = simulate_run(model, init_window, n_steps)\n",
    "            \n",
    "            md_out = dict(md_emp)\n",
    "            md_out.update({\n",
    "                \"simulated\": True,\n",
    "                \"duration_emp_s\": float(dur_s),\n",
    "                \"n_steps_model\": int(n_steps),\n",
    "                **meta_sim\n",
    "            })\n",
    "            \n",
    "            dataset_sim[sub_id][\"task-rest\"][int(run_idx)] = {\n",
    "                \"time series\": sim_ts,\n",
    "                \"metadata\": md_out\n",
    "            }\n",
    "            n_sim_rest += 1\n",
    "    \n",
    "    # ---- SIMULATE STIM ----\n",
    "    if \"task-stim\" in sub_data:\n",
    "        for run_idx, run in sub_data[\"task-stim\"].items():\n",
    "            ts_emp = run.get(\"time series\", None)\n",
    "            md_emp = run.get(\"metadata\", {}) or {}\n",
    "            target_vec = run.get(\"target\", None)\n",
    "            events_df = run.get(\"stim time\", None)\n",
    "            \n",
    "            if ts_emp is None or not isinstance(ts_emp, np.ndarray) or ts_emp.shape[1] != N:\n",
    "                continue\n",
    "            \n",
    "            target_idx = safe_target_idx(target_vec)\n",
    "            if target_idx is None:\n",
    "                continue\n",
    "            \n",
    "            onset_col = get_onset_column(events_df) if isinstance(events_df, pd.DataFrame) else None\n",
    "            if onset_col is None:\n",
    "                continue\n",
    "            \n",
    "            onsets_s = events_df[onset_col].astype(float).values\n",
    "            stim_steps = list(map_onsets_to_steps(onsets_s))\n",
    "            \n",
    "            tr_emp = float(md_emp.get(\"tr_s\", 2.4))\n",
    "            dur_s = ts_emp.shape[0] * tr_emp\n",
    "            n_steps = int(math.ceil(dur_s / TR_MODEL))\n",
    "            \n",
    "            init_window = ts_emp[:S].copy()\n",
    "            sim_ts, meta_sim = simulate_run(model, init_window, n_steps,\n",
    "                                            stim_steps=stim_steps, target_idx=target_idx, W=W)\n",
    "            \n",
    "            md_out = dict(md_emp)\n",
    "            md_out.update({\n",
    "                \"simulated\": True,\n",
    "                \"duration_emp_s\": float(dur_s),\n",
    "                \"n_steps_model\": int(n_steps),\n",
    "                \"target_idx\": int(target_idx),\n",
    "                **meta_sim\n",
    "            })\n",
    "            \n",
    "            dataset_sim[sub_id][\"task-stim\"][int(run_idx)] = {\n",
    "                \"time series\": sim_ts,\n",
    "                \"metadata\": md_out,\n",
    "                \"target\": target_vec,\n",
    "                \"stim time\": events_df,\n",
    "            }\n",
    "            n_sim_stim += 1\n",
    "\n",
    "print(f\"✓ Generated {n_sim_rest} rest runs, {n_sim_stim} stim runs\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763c7784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# SAVE SYNTHETIC DATASET\n",
    "# =========================\n",
    "print(f\"Saving synthetic dataset to {OUT_PKL}...\")\n",
    "with open(OUT_PKL, \"wb\") as f:\n",
    "    pickle.dump(dataset_sim, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(\"✓ Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddea429a",
   "metadata": {},
   "source": [
    "# Validation: Subject-Specific ΔFC Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e53b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# VALIDATION FUNCTIONS\n",
    "# =========================\n",
    "\n",
    "def fc_from_timeseries(ts, cortical_only=True):\n",
    "    \"\"\"Compute Pearson FC, optionally using cortical ROIs only.\"\"\"\n",
    "    if cortical_only:\n",
    "        ts = ts[:, 50:]  # Skip Tian 50, keep Schaefer 400\n",
    "    return np.corrcoef(ts, rowvar=False).astype(np.float32)\n",
    "\n",
    "def upper_tri_vec(mat, k=1):\n",
    "    \"\"\"Extract upper triangle as 1D vector.\"\"\"\n",
    "    iu = np.triu_indices(mat.shape[0], k=k)\n",
    "    return mat[iu]\n",
    "\n",
    "print(\"✓ Validation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11347a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# COMPUTE PER-SUBJECT ΔFC\n",
    "# =========================\n",
    "print(\"Computing per-subject ΔFC correlations...\\n\")\n",
    "\n",
    "results = {\n",
    "    'subject_deltafc_corr': {},\n",
    "    'subject_info': {},\n",
    "}\n",
    "\n",
    "for sub_id in sorted(dataset_emp.keys()):\n",
    "    # Check if we have both empirical and simulated data\n",
    "    if sub_id not in dataset_sim:\n",
    "        continue\n",
    "    \n",
    "    sub_emp = dataset_emp[sub_id]\n",
    "    sub_sim = dataset_sim[sub_id]\n",
    "    \n",
    "    rest_runs_emp = sub_emp.get(\"task-rest\", {})\n",
    "    stim_runs_emp = sub_emp.get(\"task-stim\", {})\n",
    "    rest_runs_sim = sub_sim.get(\"task-rest\", {})\n",
    "    stim_runs_sim = sub_sim.get(\"task-stim\", {})\n",
    "    \n",
    "    if len(rest_runs_emp) == 0 or len(stim_runs_emp) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Empirical rest FC\n",
    "    fc_rest_emp_list = []\n",
    "    for run in rest_runs_emp.values():\n",
    "        ts = run.get(\"time series\", None)\n",
    "        if isinstance(ts, np.ndarray) and ts.shape[1] >= 450:\n",
    "            fc_rest_emp_list.append(fc_from_timeseries(ts, cortical_only=True))\n",
    "    \n",
    "    # Simulated rest FC\n",
    "    fc_rest_sim_list = []\n",
    "    for run in rest_runs_sim.values():\n",
    "        ts = run.get(\"time series\", None)\n",
    "        if isinstance(ts, np.ndarray) and ts.shape[1] >= 450:\n",
    "            fc_rest_sim_list.append(fc_from_timeseries(ts, cortical_only=True))\n",
    "    \n",
    "    if len(fc_rest_emp_list) == 0 or len(fc_rest_sim_list) == 0:\n",
    "        continue\n",
    "    \n",
    "    FC_rest_emp = np.mean(np.stack(fc_rest_emp_list), axis=0)\n",
    "    FC_rest_sim = np.mean(np.stack(fc_rest_sim_list), axis=0)\n",
    "    \n",
    "    # Empirical stim FC\n",
    "    fc_stim_emp_list = []\n",
    "    for run in stim_runs_emp.values():\n",
    "        ts = run.get(\"time series\", None)\n",
    "        if isinstance(ts, np.ndarray) and ts.shape[1] >= 450:\n",
    "            fc_stim_emp_list.append(fc_from_timeseries(ts, cortical_only=True))\n",
    "    \n",
    "    # Simulated stim FC\n",
    "    fc_stim_sim_list = []\n",
    "    for run in stim_runs_sim.values():\n",
    "        ts = run.get(\"time series\", None)\n",
    "        if isinstance(ts, np.ndarray) and ts.shape[1] >= 450:\n",
    "            fc_stim_sim_list.append(fc_from_timeseries(ts, cortical_only=True))\n",
    "    \n",
    "    if len(fc_stim_emp_list) == 0 or len(fc_stim_sim_list) == 0:\n",
    "        continue\n",
    "    \n",
    "    FC_stim_emp = np.mean(np.stack(fc_stim_emp_list), axis=0)\n",
    "    FC_stim_sim = np.mean(np.stack(fc_stim_sim_list), axis=0)\n",
    "    \n",
    "    # Compute ΔFC\n",
    "    deltaFC_emp = FC_stim_emp - FC_rest_emp\n",
    "    deltaFC_sim = FC_stim_sim - FC_rest_sim\n",
    "    \n",
    "    # Correlate upper triangles\n",
    "    vec_emp = upper_tri_vec(deltaFC_emp, k=1)\n",
    "    vec_sim = upper_tri_vec(deltaFC_sim, k=1)\n",
    "    \n",
    "    r = pearsonr(vec_emp, vec_sim)[0]\n",
    "    \n",
    "    results['subject_deltafc_corr'][sub_id] = r\n",
    "    results['subject_info'][sub_id] = {\n",
    "        'n_rest_runs': len(rest_runs_emp),\n",
    "        'n_stim_runs': len(stim_runs_emp),\n",
    "        'deltafc_emp_magnitude': float(np.abs(deltaFC_emp).mean()),\n",
    "        'deltafc_sim_magnitude': float(np.abs(deltaFC_sim).mean()),\n",
    "    }\n",
    "    \n",
    "    print(f\"{sub_id}: r_ΔFC = {r:.4f} | emp_mag={np.abs(deltaFC_emp).mean():.4f} | sim_mag={np.abs(deltaFC_sim).mean():.4f}\")\n",
    "\n",
    "print(f\"\\n✓ Computed correlations for {len(results['subject_deltafc_corr'])} subjects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d52600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# SUMMARY STATISTICS\n",
    "# =========================\n",
    "corrs = np.array(list(results['subject_deltafc_corr'].values()))\n",
    "corrs_valid = corrs[np.isfinite(corrs)]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUBJECT-SPECIFIC ΔFC VALIDATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nN subjects: {len(corrs_valid)}\")\n",
    "print(f\"Mean r(ΔFC):   {corrs_valid.mean():.4f}\")\n",
    "print(f\"Median r(ΔFC): {np.median(corrs_valid):.4f}\")\n",
    "print(f\"Std r(ΔFC):    {corrs_valid.std():.4f}\")\n",
    "print(f\"Min r(ΔFC):    {corrs_valid.min():.4f}\")\n",
    "print(f\"Max r(ΔFC):    {corrs_valid.max():.4f}\")\n",
    "print(f\"\\nCorrelations by subject:\")\n",
    "for sub_id, r in sorted(results['subject_deltafc_corr'].items()):\n",
    "    info = results['subject_info'][sub_id]\n",
    "    print(f\"  {sub_id}: r={r:.4f} | emp_ΔFC_mag={info['deltafc_emp_magnitude']:.4f} | sim_ΔFC_mag={info['deltafc_sim_magnitude']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8360e92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# HISTOGRAM: Per-Subject ΔFC Correlations\n",
    "# =========================\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.hist(corrs_valid, bins=15, color='steelblue', edgecolor='black', alpha=0.7, linewidth=1.5)\n",
    "ax.axvline(corrs_valid.mean(), color='red', linestyle='--', linewidth=2.5, label=f'Mean = {corrs_valid.mean():.3f}')\n",
    "ax.axvline(np.median(corrs_valid), color='green', linestyle='--', linewidth=2.5, label=f'Median = {np.median(corrs_valid):.3f}')\n",
    "\n",
    "ax.set_xlabel('Per-Subject ΔFC Correlation (r)', fontsize=13)\n",
    "ax.set_ylabel('Number of Subjects', fontsize=13)\n",
    "ax.set_title('Subject-Specific ΔFC Validation: Empirical vs. Simulated', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=12, loc='upper right')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Histogram saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03fe691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# SAVE VALIDATION RESULTS\n",
    "# =========================\n",
    "summary = {\n",
    "    'n_subjects': len(corrs_valid),\n",
    "    'mean_r_deltafc': float(corrs_valid.mean()),\n",
    "    'median_r_deltafc': float(np.median(corrs_valid)),\n",
    "    'std_r_deltafc': float(corrs_valid.std()),\n",
    "    'min_r_deltafc': float(corrs_valid.min()),\n",
    "    'max_r_deltafc': float(corrs_valid.max()),\n",
    "    'per_subject_correlations': {k: float(v) for k, v in results['subject_deltafc_corr'].items()},\n",
    "    'per_subject_info': results['subject_info'],\n",
    "}\n",
    "\n",
    "with open(RESULTS_JSON, \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"✓ Saved results to {RESULTS_JSON}\")\n",
    "print(f\"\\n✅ ANALYSIS COMPLETE\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
