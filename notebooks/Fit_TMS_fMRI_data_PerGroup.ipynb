{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1ba21e2",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/grabuffo/BrainStim_ANN_fMRI_HCP/blob/main/notebooks/Fit_TMS_fMRI_data_PerGroup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e037961",
   "metadata": {},
   "source": [
    "# Fit TMS-fMRI Models Per Target Region Group\n",
    "\n",
    "Train separate population models for each target region group.\n",
    "Extends `Fit_TMS_fMRI_data.ipynb` workflow but applies it to subgroups of participants grouped by stimulation target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17490f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup cell ---\n",
    "\n",
    "# 1ï¸âƒ£ Mount Google Drive (for data)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# 2ï¸âƒ£ Clone GitHub repository (for code)\n",
    "import os, sys, subprocess, pickle, json, glob\n",
    "from collections import defaultdict\n",
    "\n",
    "repo_dir = \"/content/BrainStim_ANN_fMRI_HCP\"\n",
    "if not os.path.exists(repo_dir):\n",
    "    !git clone https://github.com/grabuffo/BrainStim_ANN_fMRI_HCP.git\n",
    "else:\n",
    "    print(\"Repo already exists âœ…\")\n",
    "\n",
    "# 3ï¸âƒ£ Define paths (TMS-fMRI)\n",
    "data_dir = \"/content/drive/MyDrive/Colab Notebooks/Brain_Stim_ANN/data\"\n",
    "preproc_dir = os.path.join(data_dir, \"preprocessed_subjects_tms_fmri\")\n",
    "\n",
    "# Where to save training outputs (per-group models)\n",
    "weights_dir_group = os.path.join(preproc_dir, \"trained_models_MLP_tms_fmri_pergroup\")\n",
    "os.makedirs(weights_dir_group, exist_ok=True)\n",
    "\n",
    "# 4ï¸âƒ£ Add repo to import path + imports\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from src import NPI\n",
    "from src.preprocessing_tms_fmri import split_last_fraction, make_inputs_targets\n",
    "\n",
    "import gc\n",
    "\n",
    "# 5ï¸âƒ£ Check device\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Running on:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"âš ï¸  GPU not detected â€” training will run on CPU.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87fe056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load grouping summary from previous notebook ---\n",
    "\n",
    "summary_pkl = os.path.join(data_dir, \"TMS_fMRI\", \"target_regions_grouping_summary.pkl\")\n",
    "print(f\"Loading grouping summary from: {summary_pkl}\")\n",
    "\n",
    "with open(summary_pkl, \"rb\") as f:\n",
    "    grouping_summary = pickle.load(f)\n",
    "\n",
    "subjects_list = grouping_summary['subjects_list']\n",
    "multi_subject_groups = grouping_summary['multi_subject_groups']\n",
    "\n",
    "print(f\"âœ… Loaded grouping summary\")\n",
    "print(f\"Total subjects: {len(subjects_list)}\")\n",
    "print(f\"Groups with â‰¥2 participants: {len(multi_subject_groups)}\")\n",
    "print(f\"\\nGroups to train:\")\n",
    "for target_id, info in sorted(multi_subject_groups.items()):\n",
    "    print(f\"  Target {target_id}: {info['subject_names']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f909ae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training parameters ---\n",
    "\n",
    "# Choose ANN architecture: 'MLP', 'CNN', 'RNN', or 'VAR'\n",
    "method = \"MLP\"\n",
    "\n",
    "# Data / model hyperparameters\n",
    "ROI_num = 450         # Tian 50 + Schaefer 400\n",
    "using_steps = 3       # S: number of past steps used to predict next step\n",
    "\n",
    "# Training hyperparameters\n",
    "batch_size = 64\n",
    "num_epochs = 50\n",
    "learning_rate = 5e-4\n",
    "l2_reg = 5e-5\n",
    "\n",
    "# Population split rule (fixed): last 10% within each participant is test\n",
    "test_fraction = 0.10\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Dataset: TMS-fMRI (task-rest only), per-group models\")\n",
    "print(f\"  Method: {method}\")\n",
    "print(f\"  Regions: {ROI_num}\")\n",
    "print(f\"  Steps: {using_steps}\")\n",
    "print(f\"  Test fraction (per-subject): {test_fraction}\")\n",
    "print(f\"  Epochs: {num_epochs}\")\n",
    "print(f\"  Batch size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da355982",
   "metadata": {},
   "source": [
    "### Notes on the split (important)\n",
    "\n",
    "We **do not** concatenate raw time series across participants.\n",
    "\n",
    "Instead:\n",
    "1. For each participant in the group, we split the **filtered signals** into train/test by taking the **last 10%** as test.\n",
    "2. We create **Inputs/Targets within each split**, so no sample ever crosses a subject boundary.\n",
    "3. We then concatenate **samples** across participants in the group to form group-specific train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b27e4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define training function ---\n",
    "\n",
    "def train_NN_fixed_test(\n",
    "    model: nn.Module,\n",
    "    X_train: np.ndarray,\n",
    "    Y_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    Y_test: np.ndarray,\n",
    "    batch_size: int = 64,\n",
    "    num_epochs: int = 50,\n",
    "    lr: float = 5e-4,\n",
    "    l2: float = 0.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a model on (X_train,Y_train) and evaluate on (X_test,Y_test) each epoch.\n",
    "    Mirrors NPI.train_NN style but avoids re-splitting internally.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    train_inputs  = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "    train_targets = torch.tensor(Y_train, dtype=torch.float32, device=device)\n",
    "\n",
    "    test_inputs  = torch.tensor(X_test, dtype=torch.float32, device=device) if X_test.size else None\n",
    "    test_targets = torch.tensor(Y_test, dtype=torch.float32, device=device) if Y_test.size else None\n",
    "\n",
    "    n_train = train_inputs.shape[0]\n",
    "    train_epoch_loss, test_epoch_loss = [], []\n",
    "\n",
    "    for ep in range(num_epochs):\n",
    "        model.train()\n",
    "        perm = torch.randperm(n_train, device=device)\n",
    "        ep_loss = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for i in range(0, n_train, batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            xb = train_inputs[idx]\n",
    "            yb = train_targets[idx]\n",
    "\n",
    "            pred = model(xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            ep_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        train_epoch_loss.append(ep_loss / max(1, n_batches))\n",
    "\n",
    "        # test\n",
    "        if test_inputs is not None and test_inputs.shape[0] > 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                pred = model(test_inputs)\n",
    "                tloss = loss_fn(pred, test_targets).item()\n",
    "            test_epoch_loss.append(tloss)\n",
    "        else:\n",
    "            test_epoch_loss.append(float(\"nan\"))\n",
    "\n",
    "        if (ep + 1) % 10 == 0 or ep == 0:\n",
    "            print(f\"Epoch {ep+1:3d}/{num_epochs} | train={train_epoch_loss[-1]:.6f} | test={test_epoch_loss[-1]:.6f}\")\n",
    "\n",
    "    return model, train_epoch_loss, test_epoch_loss\n",
    "\n",
    "print(\"âœ… Training function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e8c844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train models for each group ---\n",
    "\n",
    "group_results = {}\n",
    "\n",
    "for target_id, group_info in sorted(multi_subject_groups.items()):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training group: Target {target_id}\")\n",
    "    print(f\"Participants: {group_info['subject_names']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    subject_ids_in_group = group_info['subject_ids']\n",
    "    subject_names_in_group = group_info['subject_names']\n",
    "    \n",
    "    # Build train/test datasets for this group\n",
    "    X_train_list, Y_train_list = [], []\n",
    "    X_test_list,  Y_test_list  = [], []\n",
    "    \n",
    "    for sid in subject_ids_in_group:\n",
    "        subject_name = subjects_list[sid]\n",
    "        subject_prefix = f\"sub-{subject_name}\"\n",
    "        \n",
    "        # Load signals for this subject\n",
    "        sig_path = os.path.join(preproc_dir, f\"{subject_prefix}_signals.npy\")\n",
    "        if not os.path.exists(sig_path):\n",
    "            print(f\"  âš ï¸  {subject_prefix}_signals.npy not found, skipping\")\n",
    "            continue\n",
    "        \n",
    "        signals = np.load(sig_path)  # (T, 450)\n",
    "        \n",
    "        # Split last 10% test within subject\n",
    "        sig_train, sig_test = split_last_fraction(signals, test_fraction=test_fraction)\n",
    "        \n",
    "        # Build Inputs/Targets within each split\n",
    "        Xtr, Ytr = make_inputs_targets(sig_train, steps=using_steps)\n",
    "        Xte, Yte = make_inputs_targets(sig_test,  steps=using_steps)\n",
    "        \n",
    "        # Accumulate\n",
    "        if Xtr.shape[0] > 0:\n",
    "            X_train_list.append(Xtr); Y_train_list.append(Ytr)\n",
    "        if Xte.shape[0] > 0:\n",
    "            X_test_list.append(Xte);  Y_test_list.append(Yte)\n",
    "        \n",
    "        print(f\"  {subject_prefix} | signals: {signals.shape} | train: {Xtr.shape[0]} | test: {Xte.shape[0]}\")\n",
    "    \n",
    "    # Concatenate samples across subjects in group\n",
    "    X_train = np.concatenate(X_train_list, axis=0)\n",
    "    Y_train = np.concatenate(Y_train_list, axis=0)\n",
    "    X_test  = np.concatenate(X_test_list, axis=0) if len(X_test_list) else np.zeros((0, using_steps * ROI_num), dtype=np.float32)\n",
    "    Y_test  = np.concatenate(Y_test_list, axis=0) if len(Y_test_list) else np.zeros((0, ROI_num), dtype=np.float32)\n",
    "    \n",
    "    print(f\"\\n  Group dataset shapes:\")\n",
    "    print(f\"    X_train: {X_train.shape}, Y_train: {Y_train.shape}\")\n",
    "    print(f\"    X_test:  {X_test.shape},  Y_test:  {Y_test.shape}\")\n",
    "    \n",
    "    # Build and train model for this group\n",
    "    print(f\"\\n  Training model...\")\n",
    "    model = NPI.build_model(method, ROI_num, using_steps)\n",
    "    model, train_loss, test_loss = train_NN_fixed_test(\n",
    "        model,\n",
    "        X_train, Y_train,\n",
    "        X_test, Y_test,\n",
    "        batch_size=batch_size,\n",
    "        num_epochs=num_epochs,\n",
    "        lr=learning_rate,\n",
    "        l2=l2_reg,\n",
    "    )\n",
    "    \n",
    "    print(f\"  âœ… Training complete | Final test loss: {test_loss[-1]:.6f}\")\n",
    "    \n",
    "    # Save model\n",
    "    model_path = os.path.join(weights_dir_group, f\"target_{target_id:03d}_{method}_tms_fmri.pt\")\n",
    "    torch.save(model, model_path)\n",
    "    \n",
    "    # Save results\n",
    "    results = {\n",
    "        \"target_id\": int(target_id),\n",
    "        \"subject_ids\": subject_ids_in_group,\n",
    "        \"subject_names\": subject_names_in_group,\n",
    "        \"dataset\": \"TMS-fMRI task-rest (per-group)\",\n",
    "        \"method\": method,\n",
    "        \"ROI_num\": ROI_num,\n",
    "        \"using_steps\": using_steps,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"l2_reg\": l2_reg,\n",
    "        \"test_fraction_per_subject\": test_fraction,\n",
    "        \"n_subjects\": len(subject_ids_in_group),\n",
    "        \"n_train_samples\": int(X_train.shape[0]),\n",
    "        \"n_test_samples\": int(X_test.shape[0]),\n",
    "        \"model_path\": model_path,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"test_loss\": test_loss,\n",
    "    }\n",
    "    \n",
    "    results_path = os.path.join(weights_dir_group, f\"results_target_{target_id:03d}_{method}_tms_fmri.json\")\n",
    "    with open(results_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"  ðŸ’¾ Saved model: {os.path.basename(model_path)}\")\n",
    "    print(f\"  ðŸ’¾ Saved results: {os.path.basename(results_path)}\")\n",
    "    \n",
    "    group_results[target_id] = results\n",
    "    \n",
    "    # Clean up\n",
    "    del model, X_train, Y_train, X_test, Y_test\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"âœ… Training complete for all {len(group_results)} groups\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed711121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Summary of all groups ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for target_id in sorted(group_results.keys()):\n",
    "    res = group_results[target_id]\n",
    "    print(f\"\\nTarget {target_id} ({', '.join(res['subject_names'])})\")\n",
    "    print(f\"  Subjects: {res['n_subjects']}\")\n",
    "    print(f\"  Samples: {res['n_train_samples']} train, {res['n_test_samples']} test\")\n",
    "    print(f\"  Final test loss: {res['test_loss'][-1]:.6f}\")\n",
    "    print(f\"  Model saved: {os.path.basename(res['model_path'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e0734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualize learning curves for all groups ---\n",
    "\n",
    "n_groups = len(group_results)\n",
    "ncols = min(3, n_groups)\n",
    "nrows = (n_groups + ncols - 1) // ncols\n",
    "\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(5*ncols, 4*nrows))\n",
    "if n_groups == 1:\n",
    "    axes = [axes]\n",
    "else:\n",
    "    axes = axes.flatten()\n",
    "\n",
    "for idx, target_id in enumerate(sorted(group_results.keys())):\n",
    "    res = group_results[target_id]\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    ax.plot(res['train_loss'], label='train', linewidth=2)\n",
    "    ax.plot(res['test_loss'], label='test', linewidth=2)\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('MSE loss')\n",
    "    ax.set_title(f\"Target {target_id} | {', '.join(res['subject_names'])}\")\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(n_groups, len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Learning curves plotted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bad188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualize final test loss comparison across groups ---\n",
    "\n",
    "target_ids = sorted(group_results.keys())\n",
    "final_test_losses = [group_results[tid]['test_loss'][-1] for tid in target_ids]\n",
    "group_labels = [f\"Target {tid}\\n{', '.join(group_results[tid]['subject_names'])}\" for tid in target_ids]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bars = ax.bar(range(len(target_ids)), final_test_losses, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax.set_xlabel('Group (Target Region)')\n",
    "ax.set_ylabel('Final Test Loss')\n",
    "ax.set_title('Model Performance by Target Region Group')\n",
    "ax.set_xticks(range(len(target_ids)))\n",
    "ax.set_xticklabels(group_labels, rotation=45, ha='right')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, loss in zip(bars, final_test_losses):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{loss:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Performance comparison plotted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e7074d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save master summary of all group training results ---\n",
    "\n",
    "master_summary = {\n",
    "    'method': method,\n",
    "    'ROI_num': ROI_num,\n",
    "    'using_steps': using_steps,\n",
    "    'num_epochs': num_epochs,\n",
    "    'batch_size': batch_size,\n",
    "    'learning_rate': learning_rate,\n",
    "    'l2_reg': l2_reg,\n",
    "    'test_fraction': test_fraction,\n",
    "    'n_groups': len(group_results),\n",
    "    'group_results': group_results,\n",
    "}\n",
    "\n",
    "master_summary_path = os.path.join(weights_dir_group, \"master_summary_all_groups.pkl\")\n",
    "with open(master_summary_path, \"wb\") as f:\n",
    "    pickle.dump(master_summary, f)\n",
    "\n",
    "print(f\"ðŸ’¾ Saved master summary to: {master_summary_path}\")\n",
    "print(f\"\\nâœ… All training complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
